# Data configuration
data:
  guest:
    namespace: experiment
    name: arc_challenge
  host:
    namespace: experiment
    name: arc_challenge
# fedmkt_config.yaml

# Configuration for Lora
lora_config:
  llm:
    r: 8
    lora_alpha: 16
    lora_dropout: 0.05
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
  slm:
    - # Configuration for the first SLM model
      r: 8
      lora_alpha: 32
      lora_dropout: 0.1
      target_modules:
        - q_proj
        - v_proj
    - # Configuration for the second SLM model
      r: 8
      lora_alpha: 32
      lora_dropout: 0.1
      target_modules:
        - c_attn

# Training configuration
training:
  llm:
    global_epochs: 5
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 4
    learning_rate: 3e-5
    output_dir: "./"
    dataloader_num_workers: 4
    remove_unused_columns: false
    warmup_ratio: 0.008
    lr_scheduler_type: "cosine"
    optim: "adamw_torch"
    adam_beta1: 0.9
    adam_beta2: 0.95
    weight_decay: 0.1
    max_grad_norm: 1.0
    use_cpu: false
  slm:
    global_epochs: 5
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 4
    learning_rate: 3e-5  # Adjust learning rate for SLM models
    output_dir: "./"
    dataloader_num_workers: 4
    remove_unused_columns: false
    warmup_ratio: 0.008
    lr_scheduler_type: "cosine"
    optim: "adamw_torch"
    adam_beta1: 0.9
    adam_beta2: 0.95
    weight_decay: 0.1
    max_grad_norm: 1.0
    use_cpu: false

# Paths configuration
paths:
  process_data_output_dir: "examples/data/arc_challenge" # Please add your datasets path
  llm_pretrained_path: "Sheared-LLaMa-1.3B" # Please add your mdoel path
  slm_0_pretrained_path: "opt-1.3b" # Please add your mdoel path
  slm_1_pretrained_path: "gpt2" # Please add your mdoel path
  vocab_mapping_directory: "vocab_mapping_datas" # Please add your vocab_mapping datasets path
  slm_to_llm_vocab_mapping_paths:
    - "opt_to_llama.json"
    - "gpt2_to_llama.json"
  llm_to_slm_vocab_mapping_paths:
    - "llama_to_opt.json"
    - "llama_to_gpt2.json"

# Models configuration
models:
  slm_models:
    - ["pellm.opt", "OPT"]
    - ["pellm.gpt2", "GPT2CLM"]

# Example: Additional custom configuration
custom_config:
  some_param: "value"
  another_param: 123