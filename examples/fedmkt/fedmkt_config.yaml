# fedmkt_config.yaml

# Configuration for Lora
lora_config:
  llm:
    r: 8
    lora_alpha: 16
    lora_dropout: 0.05
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
  slm:
    - # Configuration for the first SLM model
      r: 8
      lora_alpha: 32
      lora_dropout: 0.1
      target_modules:
        - q_proj
        - v_proj
    - # Configuration for the second SLM model
      r: 8
      lora_alpha: 32
      lora_dropout: 0.1
      target_modules:
        - c_attn

# Training configuration
training:
  llm:
    global_epochs: 5
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 4
    learning_rate: 3e-5
    output_dir: "./"
    dataloader_num_workers: 4
    remove_unused_columns: false
    warmup_ratio: 0.008
    lr_scheduler_type: "cosine"
    optim: "adamw_torch"
    adam_beta1: 0.9
    adam_beta2: 0.95
    weight_decay: 0.1
    max_grad_norm: 1.0
    use_cpu: false
  slm:
    global_epochs: 5
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 4
    learning_rate: 3e-5  # Adjust learning rate for SLM models
    output_dir: "./"
    dataloader_num_workers: 4
    remove_unused_columns: false
    warmup_ratio: 0.008
    lr_scheduler_type: "cosine"
    optim: "adamw_torch"
    adam_beta1: 0.9
    adam_beta2: 0.95
    weight_decay: 0.1
    max_grad_norm: 1.0
    use_cpu: false

# Paths configuration
paths:
  process_data_output_dir: ""
  llm_pretrained_path: "Llama-2-7b-hf"
  slm_pretrained_paths:
    - "opt-1.3b"
    - "gpt2"
  vocab_mapping_directory: ""
  slm_to_llm_vocab_mapping_paths:
    - "opt_to_llama.json"
    - "gpt2_to_llama.json"
    - "llama_small_to_llama.json"
  llm_to_slm_vocab_mapping_paths:
    - "llama_to_opt.json"
    - "llama_to_gpt2.json"
    - "llama_to_llama_small"

# Models configuration
models:
  slm_models:
    - ["pellm.opt", "OPT"]
    - ["pellm.gpt2", "GPT2CLM"]

# Data configuration
data:
  guest:
    namespace: "experiment"
    name: "arc_challenge"
  host:
    namespace: "experiment"
    name: "arc_challenge"

# Example: Additional custom configuration
custom_config:
  some_param: "value"
  another_param: 123
