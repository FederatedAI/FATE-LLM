data:
  guest:
    namespace: experiment
    name: ad
  host:
    namespace: experiment
    name: ad
epoch: 1
batch_size: 4
lr: 5e-4
pretrained_model_path: bloom-560m
peft_config:
  alpha_pattern: {}
  auto_mapping: null
  base_model_name_or_path: null
  bias: none
  fan_in_fan_out: false
  inference_mode: false
  init_lora_weights: true
  layers_pattern: null
  layers_to_transform: null
  loftq_config: { }
  lora_alpha: 32
  lora_dropout: 0.1
  megatron_config: null
  megatron_core: megatron.core
  modules_to_save: null
  peft_type: LORA
  r: 8
  rank_pattern: { }
  revision: null
  target_modules:
    - query_key_value
  task_type: CAUSAL_LM
  use_rslora: false
ds_config:
  fp16:
    enabled: true
  gradient_accumulation_steps: 1
  optimizer:
    params:
      adam_w_mode: false
      lr: 5e-4
      torch_adam: true
    type: Adam
  train_micro_batch_size_per_gpu: 4
  zero_optimization:
    allgather_bucket_size: 100000000.0
    allgather_partitions: true
    contiguous_gradients: true
    offload_optimizer:
      device: cpu
    offload_param:
      device: cpu
    overlap_comm: true
    reduce_bucket_size: 100000000.0
    reduce_scatter: true
    stage: 2
