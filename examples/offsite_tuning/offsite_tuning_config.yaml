data:
  guest:
    namespace: experiment
    name: sciq
  host:
    namespace: experiment
    name: sciq

# 模型路径
pretrained_model_path: "/data/projects/fate_llm_22/fate/data/cephfs/llm/models/gpt2"

# 训练参数
training:
  batch_size: 1
  lr: 5e-5
  num_train_epochs: 1
  logging_steps: 10
  disable_tqdm: false
  dataloader_num_workers: 4

# LLM模型配置
model_config:
  emulator_layer_num: 11
  adapter_top_layer_num: 2
  adapter_bottom_layer_num: 2

# DeepSpeed配置
deepspeed_config:
  train_micro_batch_size_per_gpu: 1
  optimizer:
    type: "Adam"
    params:
      lr: 5e-5
      torch_adam: true
      adam_w_mode: false
  fp16:
    enabled: true
  gradient_accumulation_steps: 1
  zero_optimization:
    stage: 2
    allgather_partitions: true
    allgather_bucket_size: 1e8
    overlap_comm: true
    reduce_scatter: true
    reduce_bucket_size: 1e8
    contiguous_gradients: true
    offload_optimizer:
      device: "cpu"
    offload_param:
      device: "cpu"
