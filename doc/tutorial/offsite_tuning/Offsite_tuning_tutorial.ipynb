{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2345e19-83eb-4196-9606-74658c8fbdc5",
   "metadata": {},
   "source": [
    "# Offsite-tuning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1d728c-09e1-418e-8d80-53dd0ec467b1",
   "metadata": {},
   "source": [
    "In this tutorial, we'll focus on how to leverage Offsite-Tuning framework in FATE to fine-tune your LLM. You'll learn how to:\n",
    "\n",
    "1. Define models, including main models(which are at server side and will offer adapters and emulators) and submodel(which are at client side and will load adapters and emulators for local fine-tuning) compatible with Offsite-Tuning framework.\n",
    "2. Get hands-on experience with the Offsite-Tuning trainer.\n",
    "3. Define configurations for advanced setup(Using Deepspeed, offsite-tuning + federation) through FATE-pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31432345-5cce-4efa-9a9b-844f997f14ad",
   "metadata": {},
   "source": [
    "## Introduction of Offsite-tuning\n",
    "\n",
    "Offsite-Tuning is a novel approach designed for the efficient and privacy-preserving adaptation of large foundational models for specific downstream tasks. The framework allows data owners to fine-tune models locally without uploading sensitive data to the LLM owner's servers. Specifically, the LLM owner sends a lightweight \"Adapter\" and a lossy compressed \"Emulator\" to the data owner. Using these smaller components, the data owner can then fine-tune the model solely on their private data. The Adapter, once fine-tuned, is returned to the model owner and integrated back into the large model to enhance its performance on the specific dataset.\n",
    "\r\n",
    "Offsite-Tuning addresses the challenge of unequal distribution of computational power and data. It allows thLLMel owner to enhance the model's capabilities without direct access to private data, while also enabling data owners who may not have the resources to train a full-scale model to fine-tune a portion of it using less computational power. This mutually beneficial arrangement accommodates both parties involve.\r\n",
    "\r\n",
    "Beyond the standard two-party setup involving the model owner and the data ownin FATE-LLM, er, Offsite-Tunframework ing is also extendable to scenarios with multiple data owners. FATE supports multi-party Offsite-Tuning, allowing multiple data owners to fine-tune and aggregate their Adapters locally, further enhancing the flexibility and applicability of this framewrFor more details of Offsite-tuning, please refer to the [original paper](https://arxiv.org/pdf/2302.04870.pdf).\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ac467-e5df-4bf3-8571-0a477ab4612d",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "\n",
    "We strongly recommend you finish reading our NN tutorial to get familiar with Model and Dataset customizations: [NN Tutorials](https://github.com/FederatedAI/FATE/blob/master/doc/tutorial/pipeline/nn_tutorial/README.md)\n",
    "You can add python path so that you can run codes in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33516e8-0d28-4c97-bc38-ba28d60acf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "your_path_to_fate_python = '/data/projects/fate/fate/python'\n",
    "sys.path.append(your_path_to_fate_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7309281b-5956-4158-9256-d6db230e086d",
   "metadata": {},
   "source": [
    "## Define Main Model and Sub Model\n",
    "\n",
    "Main models are at server side and will provides weights of adapters and emulators to client sides, while Sub Models are at client side and will load adapters and emulators for local fine-tuning. In this chapter we will take a standard GPT2 as the example and show you how to quickly develop main model class and sub model class for offsite-tuning.\n",
    "\n",
    "### Base Classes and Interfaces\n",
    "\n",
    "The base classes for the Main and Sub Models are OffsiteTuningMainModel and OffsiteTuningSubModel, respectively. To build your own models upon these base classes, you need to:\n",
    "\n",
    "1. Implement three key interfaces: get_base_model, get_model_transformer_blocks, and forward. The get_base_model interface should return the full Main or Sub Model. Meanwhile, the get_model_transformer_blocks function should return a ModuleList of all transformer blocks present in your language model, enabling the extraction of emulators and adapters from these blocks. Finally, you're required to implement the forward process for model inference.\n",
    "\n",
    "2. Supply the parameters emulator_layer_num, adapter_top_layer_num, and adapter_bottom_layer_num to the parent class. This allows the framework to automatically generate the top and bottom adapters as well as the dropout emulator for you. Specifically, the top adapters are taken from the top of the transformer blocks, while the bottom adapters are taken from the bottom. The emulator uses a dropout emulator consistent with the paper's specifications. Once the adapter layers are removed, the emulator is formed by selecting transformer blocks at fixed intervals and finally stack them to make a dropout emulator.\n",
    "\n",
    "Our framework will automatically detect the emulator and adapters of a main model, and send them to clients. Clients' models them load the weights of emulators and adapters to get trainable models.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let us take a look of our built-in GPT-2 model. It will be easy for you to build main models and sub models based on the framework. Please notice that the GPT2LMHeadSubModel's base model is intialized from a GPTConfig, that is to say, it's weights are random and need to load pretrained weights from server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8611c115-0321-458f-b190-49dcb127a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fate_llm.model_zoo.offsite_tuning.offsite_tuning_model import OffsiteTuningSubModel, OffsiteTuningMainModel\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from torch import nn\n",
    "import torch as t\n",
    "\n",
    "\n",
    "class GPT2LMHeadMainModel(OffsiteTuningMainModel):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name_or_path,\n",
    "            emulator_layer_num: int,\n",
    "            adapter_top_layer_num: int = 2,\n",
    "            adapter_bottom_layer_num: int = 2):\n",
    "\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        super().__init__(\n",
    "            emulator_layer_num,\n",
    "            adapter_top_layer_num,\n",
    "            adapter_bottom_layer_num)\n",
    "\n",
    "    def get_base_model(self):\n",
    "        return GPT2LMHeadModel.from_pretrained(self.model_name_or_path)\n",
    "\n",
    "    def get_model_transformer_blocks(self, model: GPT2LMHeadModel):\n",
    "        return model.transformer.h\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(**x)\n",
    "\n",
    "class GPT2LMHeadSubModel(OffsiteTuningSubModel):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name_or_path,\n",
    "            emulator_layer_num: int,\n",
    "            adapter_top_layer_num: int = 2,\n",
    "            adapter_bottom_layer_num: int = 2,\n",
    "            fp16_mix_precision=False,\n",
    "            partial_weight_decay=None):\n",
    "\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.emulator_layer_num = emulator_layer_num\n",
    "        self.adapter_top_layer_num = adapter_top_layer_num\n",
    "        self.adapter_bottom_layer_num = adapter_bottom_layer_num\n",
    "        super().__init__(\n",
    "            emulator_layer_num,\n",
    "            adapter_top_layer_num,\n",
    "            adapter_bottom_layer_num,\n",
    "            fp16_mix_precision)\n",
    "        self.partial_weight_decay = partial_weight_decay\n",
    "\n",
    "    def get_base_model(self):\n",
    "        total_layer_num = self.emulator_layer_num + \\\n",
    "            self.adapter_top_layer_num + self.adapter_bottom_layer_num\n",
    "        config = GPT2Config.from_pretrained(self.model_name_or_path)\n",
    "        config.num_hidden_layers = total_layer_num\n",
    "        # initialize a model without pretrained weights\n",
    "        return GPT2LMHeadModel(config)\n",
    "\n",
    "    def get_model_transformer_blocks(self, model: GPT2LMHeadModel):\n",
    "        return model.transformer.h\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(**x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1f63f-afa7-4f09-a67e-63812ddcd801",
   "metadata": {},
   "source": [
    "We can define a server side model and a client side model that can work together in the offsite-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04870e76-11cc-4d79-a09e-b6fd16ed2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_main = GPT2LMHeadMainModel('gpt2', 4, 2, 2)\n",
    "model_sub = GPT2LMHeadSubModel('gpt2', 4, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d34937-b4ae-436e-b4ea-1620fb80bed4",
   "metadata": {},
   "source": [
    "### Share additional parameters with clients\n",
    "\n",
    "Additionally, beyond the weights of emulators and adapters, you may also want to share other model parameters, such as embedding weights, with your client partners. To achieve this, you'll need to implement two more interfaces: get_additional_param_state_dict and load_additional_param_state_dict for both the Main and Sub Models.\n",
    "\n",
    "### Special Attention for Large Objects\n",
    "\n",
    "Please note that special attention is required when you need to share large objects, any object potentially exceeding 2GB, such as embedding weights. You should slice these large objects to manage them more efficiently. Below is a code snippet demonstrating this practice, taken directly from FATE's native GPT-2 implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189fce0e-8e4d-4368-8e14-907b30ce0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_additional_param_state_dict(self):\n",
    "    # get parameter of additional parameter\n",
    "    model = self.model\n",
    "    param_dict = {\n",
    "        'wte': model.transformer.wte,\n",
    "        'wpe': model.transformer.wpe,\n",
    "        'last_ln_f': model.transformer.ln_f\n",
    "    }\n",
    "\n",
    "    addition_weights = self.get_numpy_state_dict(param_dict)\n",
    "\n",
    "    wte = addition_weights.pop('wte')\n",
    "    wte_dict = split_numpy_array(wte, 10, 'wte')\n",
    "    wpe = addition_weights.pop('wpe')\n",
    "    wpe_dict = split_numpy_array(wpe, 10, 'wpe')\n",
    "    addition_weights.update(wte_dict)\n",
    "    addition_weights.update(wpe_dict)\n",
    "    return addition_weights\n",
    "\n",
    "def load_additional_param_state_dict(self, submodel_weights: dict):\n",
    "    # load additional weights:\n",
    "    model = self.model\n",
    "    param_dict = {\n",
    "        'wte': model.transformer.wte,\n",
    "        'wpe': model.transformer.wpe,\n",
    "        'last_ln_f': model.transformer.ln_f\n",
    "    }\n",
    "\n",
    "    new_submodel_weight = {}\n",
    "    new_submodel_weight['last_ln_f'] = submodel_weights['last_ln_f']\n",
    "    wte_dict, wpe_dict = {}, {}\n",
    "    for k, v in submodel_weights.items():\n",
    "        if 'wte' in k:\n",
    "            wte_dict[k] = v\n",
    "        if 'wpe' in k:\n",
    "            wpe_dict[k] = v\n",
    "    wte = recover_numpy_array(wte_dict, 'wte')\n",
    "    wpe = recover_numpy_array(wpe_dict, 'wpe')\n",
    "    new_submodel_weight['wte'] = wte\n",
    "    new_submodel_weight['wpe'] = wpe\n",
    "\n",
    "    self.load_numpy_state_dict(param_dict, new_submodel_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9aa6a-80e9-4130-8af1-c7d2bd0fbba3",
   "metadata": {},
   "source": [
    "From these codes we can see that we use 'split_numpy_array, recover_numpy_array' to cut embedding weights into pieces and recover them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda6f5e3-d05a-4cdf-afd4-affbc162fce4",
   "metadata": {},
   "source": [
    "## Submit a Offsite-tuning Task - A QA Task Sample with GPT2\n",
    "\n",
    "Now we are going to show you how to run a 2 party(server & client) offsite-tuning task using the GPT-2 model defined above. Before we submit the task we need to prepare the QA dataset.\n",
    "\n",
    "### Prepare QA Dataset - Sciq\n",
    "\n",
    "In this example, we use sciq dataset. You can use tools provided in our qa_dataset.py to tokenize the sciq dataset and save the tokenized result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f6947e-f0a3-4a42-9549-a9776a15b66d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fate_llm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfate_llm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqa_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tokenize_qa_dataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      3\u001b[0m tokenizer_name_or_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/projects/fate/cwj/gpt2\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fate_llm'"
     ]
    }
   ],
   "source": [
    "from fate_llm.dataset.qa_dataset import tokenize_qa_dataset\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer_name_or_path = '/data/projects/fate/cwj/gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(gpt2_path)\n",
    "\n",
    "if 'llama' in tokenizer_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, unk_token=\"<unk>\",  bos_token=\"<s>\", eos_token=\"</s>\", add_eos_token=True)   \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "if 'gpt2' in tokenizer_name_or_path:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "import os\n",
    "# bind data path to name & namespace\n",
    "fate_project_path = os.path.abspath('../../../')\n",
    "rs = tokenize_qa_dataset('sciq', tokenizer, fate_project_path + '/sciq/', seq_max_len=600)  # we save the cache dataset to the fate root folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adabe89a-37be-4c64-bd83-4f8c8b80096f",
   "metadata": {},
   "source": [
    "We can use our built-in QA dataset to load tokenized dataset, to see if everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6500c2ba-bc39-4db4-b2ea-947fb09c334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fate_llm.dataset.qa_dataset import QaDataset\n",
    "\n",
    "ds = QaDataset(tokenizer_name_or_path=tokenizer_name_or_path)\n",
    "ds.load(fate_project_path + '/sciq/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6f62b60-eed0-4bd0-874e-ae3feeebb120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11679\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "print(len(ds))  # train set length\n",
    "print(ds[0]['input_ids'].__len__()) # first sample length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0609c63d-35a4-43bc-bd4b-f1c61adea587",
   "metadata": {},
   "source": [
    "## Submit a Task\n",
    "\n",
    "Now the model and the dataset is prepared! We can submit a training task. \n",
    "After we submit the task below, the following process will occur: The server and client each initialize their respective models. The server extracts shared parameters and sends them to the client. The client then loads these parameters and conducts training on a miniaturized GPT-2 model composed of an emulator and adaptesr onSciqP We speicify the OffsiteTuningTrainer via TrainerParam. If you are not familiar with trainer configuration, please refer to [FATE-NN Tutorial](https://github.com/FederatedAI/FATE/blob/master/doc/tutorial/pipeline/nn_tutorial/README.md).\n",
    " Upon completion of the training, the client sends the adapter parameters back to the server. Since we are directly using Hugging Face's LMHeadGPT2, there's no need to supply a loss function. Simply inputting the preprocessed data and labels into the model will calculate the correct loss and proceed with gradient descent\n",
    "\n",
    "One thing to pay special attention to is that Offsite-Tuning differs from FedAvg within FATE. In Offsite-Tuning, the server (the arbiter role) needs to initialize the model. Therefore, please refer to the example below and set the 'nn_component' parameters separately for the client and the server. Also, don't forget to add the 'server_init=True' parameter to the server; otherwise, the arbiter side will not initialize the model.\n",
    "\n",
    "To make this a quick demo, we only select 100 samples from the origin qa datset, see 'select_num=100' in the DatasetParam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9113d10-c3e7-4875-9502-ce46aa0b86b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pipeline.backend.pipeline.PipeLine at 0x7f81000ec850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from pipeline import fate_torch_hook\n",
    "from pipeline.component import HomoNN\n",
    "from pipeline.backend.pipeline import PipeLine\n",
    "from pipeline.component import Reader, Evaluation, DataTransform\n",
    "from pipeline.interface import Data, Model\n",
    "\n",
    "t = fate_torch_hook(t)\n",
    "\n",
    "import os\n",
    "# bind data path to name & namespace\n",
    "fate_project_path = os.path.abspath('../../../')\n",
    "guest = 9997\n",
    "arbiter = 9997\n",
    "pipeline = PipeLine().set_initiator(role='guest', party_id=guest).set_roles(guest=guest, arbiter=arbiter)\n",
    "\n",
    "# bind data path with name & namespace\n",
    "data_0 = {\"name\": \"sciq\", \"namespace\": \"experiment\"}\n",
    "data_path_0 = fate_project_path + '/sciq/'\n",
    "pipeline.bind_table(name=data_0['name'], namespace=data_0['namespace'], path=data_path_0)\n",
    "\n",
    "reader_0 = Reader(name=\"reader_0\")\n",
    "reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=data_0)\n",
    "\n",
    "gpt2_type = '/data/projects/fate/cwj/gpt2/'\n",
    "\n",
    "from pipeline.component.nn import DatasetParam\n",
    "dataset_param = DatasetParam(dataset_name='qa_dataset', tokenizer_name_or_path=gpt2_type, select_num=100)\n",
    "\n",
    "from pipeline.component.homo_nn import TrainerParam  # Interface\n",
    "sub_model_client = t.nn.CustModel(module_name='offsite_tuning.gpt2_ot', class_name='GPT2LMHeadSubModel', model_name_or_path=gpt2_type \\\n",
    "                                  ,emulator_layer_num=4, adapter_top_layer_num=2, adapter_bottom_layer_num=2)\n",
    "main_model_server = t.nn.CustModel(module_name='offsite_tuning.gpt2_ot', class_name='GPT2LMHeadMainModel', model_name_or_path=gpt2_type \\\n",
    "                                  ,emulator_layer_num=4, adapter_top_layer_num=2, adapter_bottom_layer_num=2)\n",
    "\n",
    "nn_component = HomoNN(name='nn_0')\n",
    "\n",
    "nn_component.get_party_instance(role='guest', party_id=guest).component_param(model=sub_model_client, dataset=dataset_param,  # dataset\n",
    "                                                                              trainer=TrainerParam(trainer_name='offsite_tuning_trainer', epochs=3, batch_size=4, collate_fn='DataCollatorForTokenClassification', task_type='causal_ml', \\\n",
    "                                                                                                   save_to_local_dir=True, cuda=0),\n",
    "                                                                             optimizer=t.optim.Adam(lr=5e-5)\n",
    "                                                                             )\n",
    "nn_component.get_party_instance(role='arbiter', party_id=arbiter).component_param(model=main_model_server, \n",
    "                                                                                  trainer=TrainerParam(trainer_name='offsite_tuning_trainer', collate_fn='DataCollatorForTokenClassification', save_to_local_dir=True),\n",
    "                                                                                  # Attention here\n",
    "                                                                                  server_init=True # This parameter must be set True !!!!!!!!!!!\n",
    "                                                                                )\n",
    "pipeline.add_component(reader_0)\n",
    "pipeline.add_component(nn_component, data=Data(train_data=reader_0.output.data))\n",
    "pipeline.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74497742-4030-4a7a-a13e-2c020da47cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b2e2b-3b53-4881-8db6-a67e1293e88b",
   "metadata": {},
   "source": [
    "## Add Deepspeed Setting\n",
    "\n",
    "By simply adding a ds_config, we can run our task with a deepspeed backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e8f063b-263c-4ba5-b2ba-98a86ce38b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pipeline.backend.pipeline.PipeLine at 0x7f8002385e50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from pipeline import fate_torch_hook\n",
    "from pipeline.component import HomoNN\n",
    "from pipeline.backend.pipeline import PipeLine\n",
    "from pipeline.component import Reader, Evaluation, DataTransform\n",
    "from pipeline.interface import Data, Model\n",
    "\n",
    "t = fate_torch_hook(t)\n",
    "\n",
    "import os\n",
    "# bind data path to name & namespace\n",
    "fate_project_path = os.path.abspath('../../../')\n",
    "guest = 9997\n",
    "arbiter = 9997\n",
    "pipeline = PipeLine().set_initiator(role='guest', party_id=guest).set_roles(guest=guest, arbiter=arbiter)\n",
    "\n",
    "# bind data path with name & namespace\n",
    "data_0 = {\"name\": \"sciq\", \"namespace\": \"experiment\"}\n",
    "data_path_0 = fate_project_path + '/sciq/'\n",
    "pipeline.bind_table(name=data_0['name'], namespace=data_0['namespace'], path=data_path_0)\n",
    "\n",
    "reader_0 = Reader(name=\"reader_0\")\n",
    "reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=data_0)\n",
    "\n",
    "# deepspeed config\n",
    "ds_config = {\n",
    "    \"train_micro_batch_size_per_gpu\": 2,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": 5e-5\n",
    "        }\n",
    "    }\n",
    "    ,\n",
    "    \"fp16\": {\n",
    "        \"enabled\": False\n",
    "    }\n",
    "    ,\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 1,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\"\n",
    "        },\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"overlap_comm\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "gpt2_type = '/data/projects/fate/cwj/gpt2/'\n",
    "\n",
    "from pipeline.component.nn import DatasetParam\n",
    "dataset_param = DatasetParam(dataset_name='qa_dataset', tokenizer_name_or_path=gpt2_type, select_num=100)\n",
    "\n",
    "from pipeline.component.homo_nn import TrainerParam  # Interface\n",
    "sub_model_client = t.nn.CustModel(module_name='offsite_tuning.gpt2_ot', class_name='GPT2LMHeadSubModel', model_name_or_path=gpt2_type \\\n",
    "                                  ,emulator_layer_num=4, adapter_top_layer_num=2, adapter_bottom_layer_num=2)\n",
    "main_model_server = t.nn.CustModel(module_name='offsite_tuning.gpt2_ot', class_name='GPT2LMHeadMainModel', model_name_or_path=gpt2_type \\\n",
    "                                  ,emulator_layer_num=4, adapter_top_layer_num=2, adapter_bottom_layer_num=2)\n",
    "\n",
    "nn_component = HomoNN(name='nn_0')\n",
    "\n",
    "nn_component.get_party_instance(role='guest', party_id=guest).component_param(model=sub_model_client, dataset=dataset_param,  # dataset\n",
    "                                                                              trainer=TrainerParam(trainer_name='offsite_tuning_trainer', epochs=3, batch_size=4, collate_fn='DataCollatorForTokenClassification', task_type='causal_ml', \\\n",
    "                                                                                                   save_to_local_dir=True),\n",
    "                                                                             optimizer=t.optim.Adam(lr=5e-5)\n",
    "                                                                             )\n",
    "nn_component.get_party_instance(role='arbiter', party_id=arbiter).component_param(model=main_model_server, \n",
    "                                                                                  trainer=TrainerParam(trainer_name='offsite_tuning_trainer', collate_fn='DataCollatorForTokenClassification', save_to_local_dir=True),\n",
    "                                                                                  # Attention here\n",
    "                                                                                  server_init=True # This parameter must be set True !!!!!!!!!!!\n",
    "                                                                                )\n",
    "pipeline.add_component(reader_0)\n",
    "pipeline.add_component(nn_component, data=Data(train_data=reader_0.output.data))\n",
    "pipeline.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23320cb9-d06a-44ac-8966-398b0f7bbaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.runtime.entity import JobParameters\n",
    "pipeline.fit(JobParameters(task_conf={\n",
    "    \"nn_0\": {\n",
    "        \"launcher\": \"deepspeed\",\n",
    "        \"world_size\": 4\n",
    "    }\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97249681-c3a3-43bd-8167-7ae3f4e1616b",
   "metadata": {},
   "source": [
    "## Offsite-tuning + Multi Client Federation\n",
    "\n",
    "\n",
    "The Offsite-Tuning + FedAVG federation is configured based on the standard Offsite-Tuning. The setup is a bit more complex, but we will walk you through it step by step. The pipeline code below contains detailed comments. When reading, please pay attention to the following points:\n",
    "\n",
    "1. In a multi-party scenario, please fill in different party_ids based on your deployment.\n",
    "2. The operation to bind the data path with the name & namespace needs to be run on the machines of all parties. For convenience, we've placed the code in one location.\n",
    "3. When configuring Trainer parameters, make sure to add the 'need_aggregate=True' parameter to the OffsiteTuningTrainer for each client and server. So adapters will be aggregated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbdc60c-a948-4be3-bba6-519d8640b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from pipeline import fate_torch_hook\n",
    "from pipeline.component import HomoNN\n",
    "from pipeline.backend.pipeline import PipeLine\n",
    "from pipeline.component import Reader, Evaluation, DataTransform\n",
    "from pipeline.interface import Data, Model\n",
    "\n",
    "t = fate_torch_hook(t)\n",
    "\n",
    "import os\n",
    "# bind data path to name & namespace\n",
    "fate_project_path = os.path.abspath('../../../')\n",
    "guest = 9997\n",
    "hosts = [9999, 10000]\n",
    "arbiter = 9997\n",
    "pipeline = PipeLine().set_initiator(role='guest', party_id=guest).set_roles(guest=guest, arbiter=arbiter, host=hosts)\n",
    "\n",
    "data_9997 = {\"name\": \"sciq-9997-gpt2\", \"namespace\": \"experiment\"}\n",
    "data_9999 = {\"name\": \"sciq-9999-gpt2\", \"namespace\": \"experiment\"}\n",
    "data_10000 = {\"name\": \"sciq-10000-gpt2\", \"namespace\": \"experiment\"}\n",
    "\n",
    "# run the binding codes on 9997\n",
    "data_path_9997 = fate_project_path + '/sciq/'\n",
    "pipeline.bind_table(name=data_9997['name'], namespace=data_9997['namespace'], path=data_path_9997)\n",
    "\n",
    "# run the binding codes on 9998\n",
    "data_path_9999 = fate_project_path + '/sciq/'\n",
    "pipeline.bind_table(name=data_9999['name'], namespace=data_9999['namespace'], path=data_path_9999)\n",
    "\n",
    "# run the binding codes on 10000\n",
    "data_path_10000 = fate_project_path + '/sciq/'\n",
    "pipeline.bind_table(name=data_10000['name'], namespace=data_10000['namespace'], path=data_path_10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253499d2-37a1-4fbe-9427-646d51fd6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepspeed config\n",
    "ds_config = {\n",
    "    \"train_micro_batch_size_per_gpu\": 2,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": 5e-5\n",
    "        }\n",
    "    }\n",
    "    ,\n",
    "    \"fp16\": {\n",
    "        \"enabled\": False\n",
    "    }\n",
    "    ,\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 1,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\"\n",
    "        },\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"overlap_comm\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909dc4fb-8d1e-4831-a6f7-744cf7d826c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283025d-9acf-4ffa-8a25-648aa619528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_0 = Reader(name=\"reader_0\")\n",
    "reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=data_9997)\n",
    "reader_0.get_party_instance(role='host', party_id=hosts[0]).component_param(table=data_9999)\n",
    "reader_0.get_party_instance(role='host', party_id=hosts[1]).component_param(table=data_10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1cc8a-1003-4379-aa4f-bf3fa28237c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.component.nn import DatasetParam\n",
    "\n",
    "# This demo utilizes the same dataset but selects distinct segments to mimic an equal data distribution across different parties. \n",
    "# We adopt this strategy for the sake of convenience.\n",
    "dataset_param_0 = DatasetParam(dataset_name='qa_ds', tokenizer_name_or_path=model_path, start_idx=0, select_num=3893)\n",
    "dataset_param_1 = DatasetParam(dataset_name='qa_ds', tokenizer_name_or_path=model_path, start_idx=3893, select_num=3893)\n",
    "dataset_param_2 = DatasetParam(dataset_name='qa_ds', tokenizer_name_or_path=model_path, start_idx=7786, select_num=3893)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea1168-417c-41da-b7da-b2625c26af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.component.homo_nn import TrainerParam  # Interface\n",
    "\n",
    "# define model structure\n",
    "sub_model_client = t.nn.CustModel(module_name='offsite_tuning.gpt2_ot', class_name='GPT2LMHeadSubModel', model_name_or_path=model_path \\\n",
    "                                  ,emulator_layer_num=4, adapter_top_layer_num=2, adapter_bottom_layer_num=2)\n",
    "main_model_server = t.nn.CustModel(module_name='offsite_tuning.gpt2_ot', class_name='GPT2LMHeadMainModel', model_name_or_path=model_path \\\n",
    "                                  ,emulator_layer_num=4, adapter_top_layer_num=2, adapter_bottom_layer_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffcace2-0d59-411e-856f-512e7eafd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_component = HomoNN(name='nn_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c854117-3fe1-4a7b-9505-bb131d95f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "# We have 4 party to set\n",
    "# Please make sure that need_aggregate is True, and epochs parameter of all parties are the same\n",
    "nn_component.get_party_instance(role='guest', party_id=guest).component_param(model=sub_model_client, dataset=dataset_param_0,  # dataset\n",
    "                                                                              trainer=TrainerParam(trainer_name='offsite_tuning_trainer', epochs=epochs, batch_size=4, collate_fn='DataCollatorForTokenClassification', task_type='causal_ml', \\\n",
    "                                                                                                   save_to_local_dir=True, need_aggregate=True), ds_config=ds_config)\n",
    "\n",
    "nn_component.get_party_instance(role='host', party_id=hosts[0]).component_param(model=sub_model_client, dataset=dataset_param_1,  # dataset\n",
    "                                                                              trainer=TrainerParam(trainer_name='offsite_tuning_trainer', epochs=epochs, batch_size=4, collate_fn='DataCollatorForTokenClassification', task_type='causal_ml', \\\n",
    "                                                                                                   save_to_local_dir=True, need_aggregate=True), ds_config=ds_config)\n",
    "\n",
    "nn_component.get_party_instance(role='host', party_id=hosts[1]).component_param(model=sub_model_client, dataset=dataset_param_2,  # dataset\n",
    "                                                                              trainer=TrainerParam(trainer_name='offsite_tuning_trainer', epochs=epochs, batch_size=4, collate_fn='DataCollatorForTokenClassification', task_type='causal_ml', \\\n",
    "                                                                                                   save_to_local_dir=True, need_aggregate=True), ds_config=ds_config)\n",
    "\n",
    "\n",
    "nn_component.get_party_instance(role='arbiter', party_id=arbiter).component_param(model=main_model_server,\n",
    "                                                                                  trainer=TrainerParam(trainer_name='offsite_tuning_trainer', epochs=epochs, save_to_local_dir=True,\n",
    "                                                                                                       need_aggregate=True),\n",
    "                                                                                  server_init=True\n",
    "                                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d173c1-5d72-4d25-9b78-91e6ef766d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_component(reader_0)\n",
    "pipeline.add_component(nn_component, data=Data(train_data=reader_0.output.data))\n",
    "pipeline.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6674178-2c59-43d6-b6ce-888e426f27b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.runtime.entity import JobParameters\n",
    "pipeline.fit(JobParameters(task_conf={\n",
    "    \"nn_0\": {\n",
    "        \"launcher\": \"deepspeed\",\n",
    "        \"world_size\": 4\n",
    "    }\n",
    "}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
