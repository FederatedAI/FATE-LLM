{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Federated ChatGLM Tuning with Parameter Efficient methods in FATE-LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will demonstrate how to efficiently train federated ChatGLM-6B using the FATE-LLM framework. In FATE-LLM, we introduce the \"pellm\"(Parameter Efficient Large Language Model) module, specifically designed for federated learning with large language models. We enable the implementation of parameter-efficient methods in federated learning, reducing communication overhead while maintaining model performance. In this tutorial we particularlly focus on ChatGLM-^b, and we will also emphasize the use of the Adapter mechanism for fine-tuning ChatGLM-6B, which enables us to effectively reduce communication volume and improve overall efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGLM-6B\n",
    "\n",
    "ChatGLM-6B is a large transformer-based language model with 6.2 billion parameters, trained on about 1T tokens of Chinese and English corpus. ChatGLM-6B is an open bilingual language model based on General Language Model. You can download the pretrained model from [here](https://huggingface.co/THUDM/chatglm-6b), or let the program automatically download it when you use it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Advertising Text Generation\n",
    "\n",
    "This is an advertising test generateion dataset, you can download dataset from the following links: \n",
    "- [data link 1](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view)\n",
    "- [data link 2](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1)\n",
    "and place it in the examples/data folder. \n",
    "\n",
    "You can refer to following link for more details about [data](https://aclanthology.org/D19-1321.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('${fate_install}/examples/data/AdvertiseGen/train.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤</td>\n",
       "      <td>宽松的阔腿裤这两年真的吸粉不少，明星时尚达人的心头爱。毕竟好穿时尚，谁都能穿出腿长2米的效果...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>类型#裙*风格#简约*图案#条纹*图案#线条*图案#撞色*裙型#鱼尾裙*裙袖长#无袖</td>\n",
       "      <td>圆形领口修饰脖颈线条，适合各种脸型，耐看有气质。无袖设计，尤显清凉，简约横条纹装饰，使得整身...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>类型#上衣*版型#宽松*颜色#粉红色*图案#字母*图案#文字*图案#线条*衣样式#卫衣*衣款...</td>\n",
       "      <td>宽松的卫衣版型包裹着整个身材，宽大的衣身与身材形成鲜明的对比描绘出纤瘦的身形。下摆与袖口的不...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>类型#裙*版型#宽松*材质#雪纺*风格#清新*裙型#a字*裙长#连衣裙</td>\n",
       "      <td>踩着轻盈的步伐享受在午后的和煦风中，让放松与惬意感为你免去一身的压力与束缚，仿佛要将灵魂也寄...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>类型#上衣*材质#棉*颜色#蓝色*风格#潮*衣样式#polo*衣领型#polo领*衣袖长#短...</td>\n",
       "      <td>想要在人群中脱颖而出吗？那么最适合您的莫过于这款polo衫短袖，采用了经典的polo领口和柔...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114594</th>\n",
       "      <td>类型#上衣*风格#运动*风格#休闲*衣样式#外套*衣领型#立领*衣袖长#长袖*衣门襟#拉链*...</td>\n",
       "      <td>基础的外套廓形，直筒，立领长袖，中间金属拉链穿脱，方便实用，带有浓浓的休闲运动味道。日常休闲...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114595</th>\n",
       "      <td>类型#上衣*风格#街头*图案#创意*衣样式#卫衣</td>\n",
       "      <td>在这件卫衣上，BRAND-white集合了女性化的柔美还有不变的街头风采，&lt;UNK&gt;&lt;UNK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114596</th>\n",
       "      <td>类型#裙*版型#宽松*版型#显瘦*颜色#黑色*图案#撞色*裙型#直筒裙*裙款式#拼接</td>\n",
       "      <td>采用简洁大体的黑色格调，宽松舒适的裙子内里，配上落肩的袖子拼接，不惧夏日的炎热，穿出清凉舒适...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114597</th>\n",
       "      <td>类型#上衣*颜色#黑色*颜色#紫色*风格#性感*图案#字母*图案#文字*图案#线条*图案#刺...</td>\n",
       "      <td>卫衣的短款长度设计能够适当地露出腰线，打造出纤瘦的身材十分性感。衣身的字母刺绣图案有着小巧的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114598</th>\n",
       "      <td>类型#上衣*颜色#黑白*风格#简约*风格#休闲*图案#条纹*衣样式#风衣*衣样式#外套</td>\n",
       "      <td>设计师以条纹作为风衣外套的主要设计元素，以简约点缀了外套，带来大气休闲的视觉效果。因为采用的...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114599 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  content  \\\n",
       "0                           类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤   \n",
       "1              类型#裙*风格#简约*图案#条纹*图案#线条*图案#撞色*裙型#鱼尾裙*裙袖长#无袖   \n",
       "2       类型#上衣*版型#宽松*颜色#粉红色*图案#字母*图案#文字*图案#线条*衣样式#卫衣*衣款...   \n",
       "3                     类型#裙*版型#宽松*材质#雪纺*风格#清新*裙型#a字*裙长#连衣裙   \n",
       "4       类型#上衣*材质#棉*颜色#蓝色*风格#潮*衣样式#polo*衣领型#polo领*衣袖长#短...   \n",
       "...                                                   ...   \n",
       "114594  类型#上衣*风格#运动*风格#休闲*衣样式#外套*衣领型#立领*衣袖长#长袖*衣门襟#拉链*...   \n",
       "114595                           类型#上衣*风格#街头*图案#创意*衣样式#卫衣   \n",
       "114596         类型#裙*版型#宽松*版型#显瘦*颜色#黑色*图案#撞色*裙型#直筒裙*裙款式#拼接   \n",
       "114597  类型#上衣*颜色#黑色*颜色#紫色*风格#性感*图案#字母*图案#文字*图案#线条*图案#刺...   \n",
       "114598        类型#上衣*颜色#黑白*风格#简约*风格#休闲*图案#条纹*衣样式#风衣*衣样式#外套   \n",
       "\n",
       "                                                  summary  \n",
       "0       宽松的阔腿裤这两年真的吸粉不少，明星时尚达人的心头爱。毕竟好穿时尚，谁都能穿出腿长2米的效果...  \n",
       "1       圆形领口修饰脖颈线条，适合各种脸型，耐看有气质。无袖设计，尤显清凉，简约横条纹装饰，使得整身...  \n",
       "2       宽松的卫衣版型包裹着整个身材，宽大的衣身与身材形成鲜明的对比描绘出纤瘦的身形。下摆与袖口的不...  \n",
       "3       踩着轻盈的步伐享受在午后的和煦风中，让放松与惬意感为你免去一身的压力与束缚，仿佛要将灵魂也寄...  \n",
       "4       想要在人群中脱颖而出吗？那么最适合您的莫过于这款polo衫短袖，采用了经典的polo领口和柔...  \n",
       "...                                                   ...  \n",
       "114594  基础的外套廓形，直筒，立领长袖，中间金属拉链穿脱，方便实用，带有浓浓的休闲运动味道。日常休闲...  \n",
       "114595  在这件卫衣上，BRAND-white集合了女性化的柔美还有不变的街头风采，<UNK><UNK...  \n",
       "114596  采用简洁大体的黑色格调，宽松舒适的裙子内里，配上落肩的袖子拼接，不惧夏日的炎热，穿出清凉舒适...  \n",
       "114597  卫衣的短款长度设计能够适当地露出腰线，打造出纤瘦的身材十分性感。衣身的字母刺绣图案有着小巧的...  \n",
       "114598  设计师以条纹作为风衣外套的主要设计元素，以简约点缀了外套，带来大气休闲的视觉效果。因为采用的...  \n",
       "\n",
       "[114599 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGLM-6B with Adapter\n",
    "\n",
    "In this section, we will guide you through the process of finetuning ChatGLM-6B with adapters using the FATE-LLM framework. Before starting this section, we recommend that you read through this tutorial first: [Model Customization](https://github.com/FederatedAI/FATE/blob/master/doc/tutorial/pipeline/nn_tutorial/Homo-NN-Customize-Model.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGLM model is located on fate_llm/model_zoo/chatglm.py, can be use directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert.py  bert.py     deberta.py     gpt2.py\t\t\t  __pycache__\r\n",
      "bart.py    chatglm.py  distilbert.py  parameter_efficient_llm.py  roberta.py\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../../../fate/python/fate_llm/model_zoo/pellm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly use adapters from the peft. See details for adapters on this page [Adapter Methods](https://huggingface.co/docs/peft/index) for more details. By specifying the adapter name and the adapter\n",
    "config dict we can insert adapters into our language models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "# define lora config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=['c_attn'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init ChatGLM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from pipeline import fate_torch_hook\n",
    "from pipeline.component.nn import save_to_fate_llm\n",
    "fate_torch_hook(t)\n",
    "\n",
    "model_path = \"your download chatglm path\"\n",
    "model = t.nn.Sequential(\n",
    "    t.nn.CustModel(module_name='pellm.chatglm', class_name='ChatGLMForConditionalGeneration',\n",
    "                   peft_config=lora_config.to_dict(), peft_type='LoraConfig',\n",
    "                   pretrained_path=model_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**During the training process, all weights of the pretrained language model will be frozen, and weights of adapters are traininable. Thus, FATE-LLM only train in the local training and aggregate adapters' weights in the fedederation process**\n",
    "\n",
    "Now available adapters are [Adapters Overview](https://huggingface.co/docs/peft/index) for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inint DeepSpeed Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_config = {\n",
    "    \"train_micro_batch_size_per_gpu\": 1,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"Adam\",\n",
    "        \"params\": {\n",
    "            \"lr\": 5e-4\n",
    "        }\n",
    "    },\n",
    "    \"bf16\": {\n",
    "        \"enabled\": True\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 5e8,\n",
    "        \"overlap_comm\": False,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 5e8,\n",
    "        \"contiguous_gradients\": True\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Federated Task\n",
    "To run federated task, please make sure to ues fate>=v1.11.2 and deploy it with gpu machines. To running this code, make sure training data path is already binded. The following code shoud be copy to a script and run in a command line like \"python federated_chatglm.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this script to submit the model, but submitting the model will take a long time to train and generate a long log, so we won't do it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import os\n",
    "from pipeline import fate_torch_hook\n",
    "from pipeline.component import HomoNN\n",
    "from pipeline.backend.pipeline import PipeLine\n",
    "from pipeline.component import Reader\n",
    "from pipeline.interface import Data\n",
    "from pipeline.runtime.entity import JobParameters\n",
    "\n",
    "fate_torch_hook(t)\n",
    "\n",
    "\n",
    "guest_0 = 9999\n",
    "host_1 = 10000\n",
    "pipeline = PipeLine().set_initiator(role='guest', party_id=guest_0).set_roles(guest=guest_0, host=host_1,\n",
    "                                                                              arbiter=guest_0)\n",
    "data_guest = {\"name\": \"ad_guest\", \"namespace\": \"experiment\"}\n",
    "data_host = {\"name\": \"ad_host\", \"namespace\": \"experiment\"}\n",
    "guest_data_path = \"${fate_install}/examples/data/AdvertiseGen/train.json_guest\"\n",
    "host_data_path = \"${fate_install}/examples/data/AdvertiseGen/train.json_host\"\n",
    "# make sure the guest and host's training data are already binded. beforem\n",
    "\n",
    "reader_0 = Reader(name=\"reader_0\")\n",
    "reader_0.get_party_instance(role='guest', party_id=guest_0).component_param(table=data_guest)\n",
    "reader_0.get_party_instance(role='host', party_id=host_1).component_param(table=data_host)\n",
    "\n",
    "## Add your pretriained model path here, will load model&tokenizer from this path\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=['query_key_value'],\n",
    ")\n",
    "ds_config = {\n",
    "    \"train_micro_batch_size_per_gpu\": 1,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"Adam\",\n",
    "        \"params\": {\n",
    "            \"lr\": 5e-4\n",
    "        }\n",
    "    },\n",
    "    \"fp16\": {\n",
    "        \"enabled\": True\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 5e8,\n",
    "        \"overlap_comm\": False,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 5e8,\n",
    "        \"contiguous_gradients\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "model_path = \"your download chatglm path\"\n",
    "from pipeline.component.homo_nn import DatasetParam, TrainerParam\n",
    "model = t.nn.Sequential(\n",
    "    t.nn.CustModel(module_name='pellm.chatglm', class_name='ChatGLMForConditionalGeneration',\n",
    "                   peft_config=lora_config.to_dict(), peft_type='LoraConfig',\n",
    "                   pretrained_path=model_path)\n",
    ")\n",
    "\n",
    "# DatasetParam\n",
    "dataset_param = DatasetParam(dataset_name='glm_tokenizer', text_max_length=64, tokenizer_name_or_path=model_path,\n",
    "                             padding_side=\"left\")\n",
    "# TrainerParam\n",
    "trainer_param = TrainerParam(trainer_name='fedavg_trainer', epochs=5, batch_size=4, checkpoint_save_freqs=1, pin_memory=False, task_type=\"seq_2_seq_lm\",\n",
    "                             data_loader_worker=8, secure_aggregate=False, save_to_local_dir=True, # pay attention to tihs parameter\n",
    "                             collate_fn=\"DataCollatorForSeq2Seq\")\n",
    "\n",
    "\n",
    "nn_component = HomoNN(name='nn_0', model=model , ds_config=ds_config)\n",
    "\n",
    "# set parameter for client 1\n",
    "nn_component.get_party_instance(role='guest', party_id=guest_0).component_param(\n",
    "    dataset=dataset_param,\n",
    "    trainer=trainer_param,\n",
    "    torch_seed=100\n",
    ")\n",
    "\n",
    "# set parameter for client 2\n",
    "nn_component.get_party_instance(role='host', party_id=host_1).component_param(\n",
    "    dataset=dataset_param,\n",
    "    trainer=trainer_param,\n",
    "    torch_seed=100\n",
    ")\n",
    "\n",
    "# set parameter for server\n",
    "nn_component.get_party_instance(role='arbiter', party_id=guest_0).component_param(\n",
    "    trainer=trainer_param\n",
    ")\n",
    "\n",
    "pipeline.add_component(reader_0)\n",
    "pipeline.add_component(nn_component, data=Data(train_data=reader_0.output.data))\n",
    "pipeline.compile()\n",
    "\n",
    "pipeline.fit(JobParameters(task_conf={\n",
    "    \"nn_0\": {\n",
    "        \"launcher\": \"deepspeed\",\n",
    "        \"world_size\": 8 # world_size means num of gpus to train in a single client\n",
    "    }\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training With P-Tuning V2 Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use another adapter lke P-Tuning V2, slightly changes is needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.component.homo_nn import DatasetParam, TrainerParam\n",
    "model = t.nn.Sequential(\n",
    "    t.nn.CustModel(module_name='pellm.chatglm', class_name='ChatGLMForConditionalGeneration',\n",
    "                   pre_seq_len=128, # only this parameters is needed\n",
    "                   pretrained_path=model_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models trained with FATE-LLM can be find under the directory `${fate_install}/fateflow/model/$jobids/$cpn_name/{model.pkl, checkpoint_xxx.pkl/adapter_model.bin}`, users must may sure \"save_to_local_dir=True\".  \n",
    "The following code is an example to load trained lora adapter weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "def load_model(pretrained_model_path):\n",
    "    _tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path, trust_remote_code=True)\n",
    "    _model = AutoModel.from_pretrained(pretrained_model_path, trust_remote_code=True)\n",
    "\n",
    "    _model = _model.half()\n",
    "    _model = _model.eval()\n",
    "\n",
    "    return _model, _tokenizer\n",
    "\n",
    "\n",
    "def load_data(data_path):\n",
    "    with open(data_path, \"r\") as fin:\n",
    "        for _l in fin:\n",
    "            yield json.loads(_l.strip())\n",
    "\n",
    "chatglm_model_path = \"\"\n",
    "model, tokenizer = load_model(chatglm_model_path)\n",
    "\n",
    "test_data_path = \"{fate_install}/examples/data/AdvertiseGen/dev.json\"\n",
    "dataset = load_data(test_data_path)\n",
    "\n",
    "peft_path = trained_model_path\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=['query_key_value'],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.load_state_dict(torch.load(peft_path), strict=False)\n",
    "model = model.half()\n",
    "model.eval()\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        print(p)\n",
    "\n",
    "model.cuda(\"cuda:0\")\n",
    "\n",
    "content = \"advertisement keywords\"\n",
    "model.chat(tokenizer, content, do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
