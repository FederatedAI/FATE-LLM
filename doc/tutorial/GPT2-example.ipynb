{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Federated GPT-2 Tuning with Parameter Efficient methods in FATE-LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will demonstrate how to efficiently train federated large language models using the FATE-LLM framework. In FATE-LLM, we introduce the \"pellm\"(Parameter Efficient Large Language Model) module, specifically designed for federated learning with large language models. We enable the implementation of parameter-efficient methods in federated learning, reducing communication overhead while maintaining model performance. In this tutorial we particularlly focus on GPT-2, and we will also emphasize the use of the Adapter mechanism for fine-tuning GPT-2, which enables us to effectively reduce communication volume and improve overall efficiency.\n",
    "\n",
    "By following this tutorial, you will learn how to leverage the FATE-LLM framework to rapidly fine-tune federated large language models, such as GPT-2, with ease and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "\n",
    "GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a causal language modeling (CLM) objective, conditioning on a left-to-right context window of 1024 tokens. In this tutorial, we will use GPT2, you can download the pretrained model from [here](https://huggingface.co/gpt2) (We choose the smallest version for this tutorial), or let the program automatically download it when you use it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: IMDB Sentimental\n",
    "\n",
    "In this section, we will introduce the process of preparing the IMDB dataset for use in our federated learning task. We use our tokenizer dataset(based on HuggingFace tokenizer) to preprocess the text data.\n",
    "\n",
    "About IMDB Sentimental Dataset:\n",
    "\n",
    "This is an binary classification dataset, you can download our processed dataset from here: \n",
    "- https://webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com/fate/examples/data/IMDB.csv\n",
    "and place it in the examples/data folder. \n",
    "\n",
    "The orgin data is from: \n",
    "- https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "### Check Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../../../examples/data/IMDB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>THE CELL (2000) Rating: 8/10&lt;br /&gt;&lt;br /&gt;The Ce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>This movie, despite its list of B, C, and D li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>I loved this movie! It was all I could do not ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>This was the worst movie I have ever seen Bill...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>2000</td>\n",
       "      <td>Stranded in Space (1972) MST3K version - a ver...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2001 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  label\n",
       "0        0  One of the other reviewers has mentioned that ...      1\n",
       "1        1  A wonderful little production. <br /><br />The...      1\n",
       "2        2  I thought this was a wonderful way to spend ti...      1\n",
       "3        3  Basically there's a family where a little boy ...      0\n",
       "4        4  Petter Mattei's \"Love in the Time of Money\" is...      1\n",
       "...    ...                                                ...    ...\n",
       "1996  1996  THE CELL (2000) Rating: 8/10<br /><br />The Ce...      1\n",
       "1997  1997  This movie, despite its list of B, C, and D li...      0\n",
       "1998  1998  I loved this movie! It was all I could do not ...      1\n",
       "1999  1999  This was the worst movie I have ever seen Bill...      0\n",
       "2000  2000  Stranded in Space (1972) MST3K version - a ver...      0\n",
       "\n",
       "[2001 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fate_llm.dataset.nlp_tokenizer import TokenizerDataset\n",
    "\n",
    "ds = TokenizerDataset(tokenizer_name_or_path=\"your model path\", text_max_length=128, \n",
    "                      padding_side=\"left\", return_input_ids=False, \n",
    "                      pad_token='<|endoftext|>')  # load tokenizer config from local pretrained tokenizer\n",
    "\n",
    "ds.load('../../../examples/data/IMDB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([ 3198,   286,   262,   584, 30702,   468,  4750,   326,   706,  4964,\n",
       "            655,   352, 18024,  4471,   345,  1183,   307, 23373,    13,  1119,\n",
       "            389,   826,    11,   355,   428,   318,  3446,   644,  3022,   351,\n",
       "            502, 29847,  1671,  1220,  6927,  1671, 11037,   464,   717,  1517,\n",
       "            326,  7425,   502,   546, 18024,   373,   663, 24557,   290, 42880,\n",
       "           8589,   278,  8188,   286,  3685,    11,   543,   900,   287,   826,\n",
       "            422,   262,  1573, 10351,    13,  9870,   502,    11,   428,   318,\n",
       "            407,   257,   905,   329,   262, 18107,  2612,   276,   393, 44295,\n",
       "             13,   770,   905, 16194,   645, 25495,   351, 13957,   284,  5010,\n",
       "             11,  1714,   393,  3685,    13,  6363,   318, 22823,    11,   287,\n",
       "            262,  6833,   779,   286,   262,  1573, 29847,  1671,  1220,  6927,\n",
       "           1671, 11037,  1026,   318,  1444,   440,    57,   355,   326,   318,\n",
       "            262, 21814,  1813,   284,   262, 34374, 22246,  4765]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1])},\n",
       " array([1.], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details of FATE-LLM dataset setting, we recommend that you read through these tutorials first: [NN Dataset Customization](https://github.com/FederatedAI/FATE/blob/master/doc/tutorial/pipeline/nn_tutorial/Homo-NN-Customize-your-Dataset.ipynb), [Some Built-In Dataset](https://github.com/FederatedAI/FATE/blob/master/doc/tutorial/pipeline/nn_tutorial/Introduce-Built-In-Dataset.ipynb),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PELLM Model with Adapter\n",
    "\n",
    "In this section, we will guide you through the process of building a parameter-efficient language model using the FATE-LLM framework. We will focus on the implementation of the PELLM model and the integration of the Adapter mechanism, which enables efficient fine-tuning and reduces communication overhead in federated learning settings. Take GPT-2 as example you will learn how to leverage the FATE-LLM framework to rapidly develop and deploy a parameter-efficient language model using FATE-LLM built-in classes. Before starting this section, we recommend that you read through this tutorial first: [Model Customization](https://github.com/FederatedAI/FATE/blob/master/doc/tutorial/pipeline/nn_tutorial/Homo-NN-Customize-Model.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PELLM Models\n",
    "\n",
    "In this section we introduce the PELLM model, which is a parameter-efficient language model that can be used in federated learning settings. They are designed to be compatible with the FATE-LLM framework to enable federated model tuning/training.\n",
    "\n",
    "PELLM models are located at federatedml.nn.model_zoo.pellm(federatedml/nn/model_zoo/pellm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert.py  bert.py     deberta.py     gpt2.py\t\t\t  __pycache__\r\n",
      "bart.py    chatglm.py  distilbert.py  parameter_efficient_llm.py  roberta.py\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../../../fate/python/fate_llm/model_zoo/pellm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize your GPT2 model by loading the pretrained model from the model folder, or downloading the pretrained model from the Huggingface,\n",
    "here we initialize the GPT2 model with the Lora Adapter, we will introduce Adapters in the following sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly use adapters from the peft. See details for adapters on this page [Adapter Methods](https://huggingface.co/docs/peft/index#supported-methods) for more details. By specifying the adapter name and the adapter\n",
    "config dict we can insert adapters into our language models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "# define lora config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=['c_attn'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init PELLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fate_llm.model_zoo.pellm.gpt2 import GPT2\n",
    "\n",
    "# case 1 load pretrained weights from local pretrained weights, same as using the huggingface pretrained model\n",
    "path_to_pretrained_folder = 'your model path'\n",
    "gpt2 = GPT2(pretrained_path=path_to_pretrained_folder, \n",
    "            peft_type=\"LoraConfig\", peft_config=lora_config.to_dict(), \n",
    "            num_labels=1, pad_token_id=50256)\n",
    "\n",
    "# case 2 directly download models from huggingface\n",
    "# gpt2 = GPT2(pretrained_path=\"gpt2\", \n",
    "#             peft_type=\"LoraConfig\", peft_config=lora_config, \n",
    "#             num_labels=1, pad_token_id=50256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version we currently support these language model for federated training:\n",
    "- ChatGLM\n",
    "- Bert\n",
    "- ALBert\n",
    "- RoBerta\n",
    "- GPT-2\n",
    "- Bart\n",
    "- DeBerta\n",
    "- DistillBert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**During the training process, all weights of the pretrained language model exclusive classifier head's weihgts will be frozen, and weights of adapters are traininable. Thus, FATE-LLM only train in the local training and aggregate adapters' weights and classifier head's weights(If has) in the fedederation process**\n",
    "\n",
    "Now available adapters are [Adapters Overview](https://huggingface.co/docs/peft/index) for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PELLM Model in FATE with CustModel\n",
    "\n",
    "In this [Model Customization](https://github.com/FederatedAI/FATE/blob/master/doc/tutorial/pipeline/nn_tutorial/Homo-NN-Customize-Model.ipynb) tutorial, we demonstrate how to employ the t.nn.CustomModel class in fate_torch to parse a model's structure and submit it to a federated learning task. The CustomModel automatically imports the model class from the model_zoo and initializes the models with the parameters provided. Since these language models are built-in, we can directly use them in the CustomModel and easily add a classifier head to address the classification task at handï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from pipeline import fate_torch_hook\n",
    "from pipeline.component.nn import save_to_fate_llm\n",
    "fate_torch_hook(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_to_fate_llm model sigmoid.py\n",
    "\n",
    "import torch as t\n",
    "\n",
    "class Sigmoid(t.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = t.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(x.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build CustModel with PELLM, and add a classifier head\n",
    "from transformers import GPT2Config\n",
    "\n",
    "checkpoint_path = \"your model path\"\n",
    "model = t.nn.Sequential(\n",
    "    t.nn.CustModel(module_name='pellm.gpt2', class_name='GPT2', \n",
    "                   pretrained_path=checkpoint_path, \n",
    "                   peft_config=lora_config.to_dict(), peft_type=\"LoraConfig\", \n",
    "                   num_labels=1,  pad_token_id=50256),\n",
    "    t.nn.CustModel(module_name='sigmoid', class_name='Sigmoid')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Please note that during the training process, only trainable parameters will participate in the federated learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Test\n",
    "\n",
    "Before submitting a federated learning task, we will demonstrate how to perform local testing to ensure the proper functionality of your custom dataset, model. We use the local mode of our FedAVGTrainer to test if our setting can run correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fate_llm.model_zoo.pellm.gpt2 import GPT2\n",
    "from fate_llm.model_zoo.sigmoid import Sigmoid\n",
    "from federatedml.nn.homo.trainer.fedavg_trainer import FedAVGTrainer\n",
    "from transformers import GPT2Config\n",
    "from fate_llm.dataset.nlp_tokenizer import TokenizerDataset\n",
    "\n",
    "# load dataset\n",
    "ds = TokenizerDataset(tokenizer_name_or_path=\"your model path\", text_max_length=128, \n",
    "                      padding_side=\"left\", return_input_ids=False, pad_token='<|endoftext|>')  # you can load tokenizer config from local pretrained tokenizer\n",
    "\n",
    "ds.load('../../../examples/data/IMDB.csv')\n",
    "\n",
    "checkpoint_path = \"your model path\"\n",
    "model = t.nn.Sequential(\n",
    "    GPT2(pretrained_path=checkpoint_path, peft_config=lora_config.to_dict(), peft_type=\"LoraConfig\", num_labels=1,  pad_token_id=50256),\n",
    "    Sigmoid()\n",
    ")\n",
    "\n",
    "trainer = FedAVGTrainer(epochs=1, batch_size=8, shuffle=True, data_loader_worker=8)\n",
    "trainer.local_mode()\n",
    "trainer.set_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch is 0\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [04:39<00:00,  1.11s/it]\n",
      "epoch loss is 0.5148034488660345\n"
     ]
    }
   ],
   "source": [
    "opt = t.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = t.nn.BCELoss()\n",
    "# local test, here we only use CPU for training\n",
    "trainer.train(ds, None, opt, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Federated Task\n",
    "Once you have successfully completed local testing, We can submit a task to FATE. Please notice that this tutorial is ran on a standalone version. **Please notice that in this tutorial we are using a standalone version, if you are using a cluster version, you need to bind the data with the corresponding name&namespace on each machine.**\n",
    "\n",
    "In this example we load pretrained weights for gpt2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import os\n",
    "from pipeline import fate_torch_hook\n",
    "from pipeline.component import HomoNN\n",
    "from pipeline.component.homo_nn import DatasetParam, TrainerParam\n",
    "from pipeline.backend.pipeline import PipeLine\n",
    "from pipeline.component import Reader\n",
    "from pipeline.interface import Data\n",
    "from transformers import GPT2Config\n",
    "\n",
    "\n",
    "fate_torch_hook(t)\n",
    "\n",
    "\n",
    "fate_project_path = \"your model path\"\n",
    "guest_0 = 9999\n",
    "host_1 = 9999\n",
    "pipeline = PipeLine().set_initiator(role='guest', party_id=guest_0).set_roles(guest=guest_0, host=host_1,\n",
    "                                                                              arbiter=guest_0)\n",
    "data_0 = {\"name\": \"imdb\", \"namespace\": \"experiment\"}\n",
    "data_path = fate_project_path + '/examples/data/IMDB.csv'\n",
    "pipeline.bind_table(name=data_0['name'], namespace=data_0['namespace'], path=data_path)\n",
    "pipeline.bind_table(name=data_0['name'], namespace=data_0['namespace'], path=data_path)\n",
    "reader_0 = Reader(name=\"reader_0\")\n",
    "reader_0.get_party_instance(role='guest', party_id=guest_0).component_param(table=data_0)\n",
    "reader_0.get_party_instance(role='host', party_id=host_1).component_param(table=data_0)\n",
    "\n",
    "reader_1 = Reader(name=\"reader_1\")\n",
    "reader_1.get_party_instance(role='guest', party_id=guest_0).component_param(table=data_0)\n",
    "reader_1.get_party_instance(role='host', party_id=host_1).component_param(table=data_0)\n",
    "\n",
    "\n",
    "## Add your pretriained model path here, will load model&tokenizer from this path\n",
    "\n",
    "\n",
    "## LoraConfig\n",
    "from peft import LoraConfig, TaskType\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=['c_attn']\n",
    ")\n",
    "\n",
    "\n",
    "model_path = 'your model path'\n",
    "model = t.nn.Sequential(\n",
    "    t.nn.CustModel(module_name='pellm.gpt2', class_name='GPT2', pretrained_path=model_path,\n",
    "                   peft_config=lora_config.to_dict(), peft_type=\"LoraConfig\", num_labels=1,  pad_token_id=50256),\n",
    "    t.nn.CustModel(module_name='sigmoid', class_name='Sigmoid')\n",
    ")\n",
    "\n",
    "# DatasetParam\n",
    "dataset_param = DatasetParam(dataset_name='nlp_tokenizer',text_max_length=128, tokenizer_name_or_path=model_path, \n",
    "                             padding_side=\"left\", return_input_ids=False, pad_token='<|endoftext|>')\n",
    "# TrainerParam\n",
    "trainer_param = TrainerParam(trainer_name='fedavg_trainer', epochs=1, batch_size=8,\n",
    "                             data_loader_worker=8)\n",
    "\n",
    "\n",
    "nn_component = HomoNN(name='nn_0', model=model)\n",
    "\n",
    "# set parameter for client 1\n",
    "nn_component.get_party_instance(role='guest', party_id=guest_0).component_param(\n",
    "    loss=t.nn.BCELoss(),\n",
    "    optimizer = t.optim.Adam(lr=0.0001, eps=1e-8),\n",
    "    dataset=dataset_param,       \n",
    "    trainer=trainer_param,\n",
    "    torch_seed=100 \n",
    ")\n",
    "\n",
    "# set parameter for client 2\n",
    "nn_component.get_party_instance(role='host', party_id=host_1).component_param(\n",
    "    loss=t.nn.BCELoss(),\n",
    "    optimizer = t.optim.Adam(lr=0.0001, eps=1e-8),\n",
    "    dataset=dataset_param,       \n",
    "    trainer=trainer_param,\n",
    "    torch_seed=100 \n",
    ")\n",
    "\n",
    "# set parameter for server\n",
    "nn_component.get_party_instance(role='arbiter', party_id=guest_0).component_param(    \n",
    "    trainer=trainer_param\n",
    ")\n",
    "\n",
    "pipeline.add_component(reader_0)\n",
    "pipeline.add_component(nn_component, data=Data(train_data=reader_0.output.data))\n",
    "pipeline.compile()\n",
    "\n",
    "pipeline.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this script to submit the model, but submitting the model will take a long time to train and generate a long log, so we won't do it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with CUDA\n",
    "\n",
    "You can use GPU by setting the cuda parameter of the FedAVGTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_param = TrainerParam(trainer_name='fedavg_trainer', epochs=1, batch_size=8, \n",
    "                             data_loader_worker=8, cuda=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cuda parameter here accepts an integer value that corresponds to the index of the GPU you want to use for training. \n",
    "In the example above, the value is set to 0, which means that on every client the first available GPU in the system will be used. \n",
    "If you have multiple GPUs and would like to use a specific one, simply change the value of the cuda parameter to the appropriate index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
