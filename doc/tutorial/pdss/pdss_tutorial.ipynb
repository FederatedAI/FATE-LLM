{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9234355d-389f-484f-9fc2-7b17563b3390",
   "metadata": {},
   "source": [
    "# PDSS Tutorial\n",
    "\n",
    "## Introduction to PDSS\n",
    "\n",
    "PDSS is a novel framework designed to distill knowledge from large language models (LLMs) to small language models (SLMs) while ensuring data privacy. The framework addresses two major challenges faced by LLM deployment in real-world applications: the privacy of domain-specific knowledge and resource constraints.\n",
    "\n",
    "PDSS adopts a server-client architecture where the client sends perturbed prompts to the server-side LLM for inference, generating perturbed rationales. The client then decodes these rationales and uses them to enrich the training of its task-specific SLM, ultimately enhancing its performance.\n",
    "\n",
    "PDSS introduces two privacy protection strategies: \n",
    "- **the Exponential Mechanism Strategy**\n",
    "- **the Encoder-Decoder Strategy**\n",
    "  \n",
    "The Exponential Mechanism Strategy utilizes a DP(differential privacy) based exponential mechanism to obfuscate user prompts, while the Encoder-Decoder Strategy employs a specialized Encoder-Decoder SLM to encode and decode perturbed prompts and rationales. These strategies effectively balance user privacy and the usability of rationales, allowing for secure and enhanced training of the client's SLM without compromising on privacy concerns.\n",
    "\n",
    "Through experiments on various text generation tasks, PDSS demonstrates its effectiveness in training task-specific SLMs with enhanced performance, significantly improving the SLM's capabilities while prioritizing data privacy protection. For more details, please refer to the [original paper](https://arxiv.org/pdf/2406.12403).\n",
    "\n",
    "**Before reading this tutorial, we strongly recommend that you first read [the InferDPT](./) tutorial.**\n",
    "\n",
    "## Use the Infer Client & Server\n",
    "\n",
    "In this section, we are going to introduce the inference part, which is the key part of PDSS that generates useful rationales with privacy-preserving. You can use InferDPT(which utilize the Exponential Mechanism Strategy) or specifically trained SLM as the text encoder & decoder. In this section, we retrieve a sample from the arc-easy dataset as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c443c920-31ff-446a-801f-d7a02409a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = {'id': 'Mercury_7220990',\n",
    "'question': 'Which factor will most likely cause a person to develop a fever?',\n",
    "'choices': {'text': ['a leg muscle relaxing after exercise',\n",
    "'a bacterial population in the bloodstream',\n",
    "'several viral particles on the skin',\n",
    "'carbohydrates being digested in the stomach'],\n",
    "'label': ['A', 'B', 'C', 'D']},\n",
    "'answerKey': 'B'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46646b18-46bb-476d-8b1d-1ef661446929",
   "metadata": {},
   "source": [
    "### Fate Context\n",
    "\n",
    "We need to create fate context to enable the communication between client and server. Then, we can initialize infer client(who will encodes the raw prompt and decodes the perturbed response) and server(who deploys the LLM) to enable secure inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cc8e8f8-88d7-45ab-a988-5ead06356418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arbiter = (\"arbiter\", 10000)\n",
    "guest = (\"guest\", 10000)\n",
    "host = (\"host\", 9999)\n",
    "name = \"fed1\"\n",
    "\n",
    "def create_ctx(local):\n",
    "    from fate.arch import Context\n",
    "    from fate.arch.computing.backends.standalone import CSession\n",
    "    from fate.arch.federation.backends.standalone import StandaloneFederation\n",
    "    import logging\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(console_handler)\n",
    "    computing = CSession(data_dir=\"./session_dir\")\n",
    "    return Context(computing=computing, federation=StandaloneFederation(computing, name, local, [guest, host, arbiter]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75dbcda-1a40-421d-ab1b-92eca5600866",
   "metadata": {},
   "source": [
    "### The DP based Strategy(InferDPT)\n",
    "\n",
    "As outlined in the [InferDPT tutorial](./), you can initialize the InferDPT client and server to facilitate secure and private inference. Prior to executing the InferDPT component, it is recommended to generate the InferDPT kit by following the step-by-step instructions provided in the tutorial.\n",
    "\n",
    "#### Client-Side Code\n",
    "\n",
    "On the client side, we load the pre-computed inferdpt-kit and deploy a local SLM as the decoding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f317f-414f-4b9f-84e6-b992b31350cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fate_llm.algo.inferdpt.inference.api import APICompletionInference\n",
    "from fate_llm.algo.inferdpt import inferdpt\n",
    "from fate_llm.algo.inferdpt.utils import InferDPTKit\n",
    "from fate_llm.algo.inferdpt.inferdpt import InferDPTClient, InferDPTServer\n",
    "from jinja2 import Template\n",
    "from fate.arch import Context\n",
    "import sys\n",
    "\n",
    "arbiter = (\"arbiter\", 10000)\n",
    "guest = (\"guest\", 10000)\n",
    "host = (\"host\", 9999)\n",
    "name = \"fed1\"\n",
    "\n",
    "def create_ctx(local):\n",
    "    from fate.arch import Context\n",
    "    from fate.arch.computing.backends.standalone import CSession\n",
    "    from fate.arch.federation.backends.standalone import StandaloneFederation\n",
    "    import logging\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(console_handler)\n",
    "    computing = CSession(data_dir=\"./session_dir\")\n",
    "    return Context(computing=computing, federation=StandaloneFederation(computing, name, local, [guest, host, arbiter]))\n",
    "\n",
    "ctx = create_ctx(guest)\n",
    "save_kit_path = 'your path'\n",
    "kit = InferDPTKit.load_from_path(save_kit_path)\n",
    "# local deployed small model as decoding model\n",
    "inference = APICompletionInference(api_url=\"http://127.0.0.1:8887/v1\", model_name='./Qwen1.5-0.5B', api_key='EMPTY')\n",
    "\n",
    "test_example = {'id': 'Mercury_7220990',\n",
    "'question': 'Which factor will most likely cause a person to develop a fever?',\n",
    "'choices': {'text': ['a leg muscle relaxing after exercise',\n",
    "'a bacterial population in the bloodstream',\n",
    "'several viral particles on the skin',\n",
    "'carbohydrates being digested in the stomach'],\n",
    "'label': ['A', 'B', 'C', 'D']},\n",
    "'answerKey': 'B'}\n",
    "\n",
    "\n",
    "doc_template = \"\"\"{{question}} \n",
    "Choices:{{choices.text}}\n",
    "\"\"\"\n",
    "\n",
    "instruction_template=\"\"\"\n",
    "<s>[INST]\n",
    "Select Answer from Choices and explain it in \"Rationale\" with few words. Please refer to the example to write the rationale.\n",
    "Use <end> to finish your rationle.\"\n",
    "\n",
    "Example(s):\n",
    "Question:George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?\n",
    "Choices:['dry palms', 'wet palms', 'palms covered with oil', 'palms covered with lotion']\n",
    "Rationale:Friction between two surfaces generates heat due to the conversion of kinetic energy into thermal energy. Dry palms produce the most heat when rubbed together as they create higher friction compared to wet or lubricated palms, which reduce friction.  Therefore, the answer is 'dry palms'.<end>\n",
    "\n",
    "Please explain:\n",
    "Question:{{perturbed_doc}}\n",
    "Rationale:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "decode_template = \"\"\"Select Answer from Choices and explain it in \"Rationale\" with few words. Please refer to the example to write the rationale.\n",
    "Use <end> to finish your rationle.\"\n",
    "\n",
    "Example(s):\n",
    "Question:George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?\n",
    "Choices:['dry palms', 'wet palms', 'palms covered with oil', 'palms covered with lotion']\n",
    "Rationale:Friction between two surfaces generates heat due to the conversion of kinetic energy into thermal energy. Dry palms produce the most heat when rubbed together as they create higher friction compared to wet or lubricated palms, which reduce friction.  Therefore, the answer is 'dry palms'.<end>\n",
    "\n",
    "Question:{{perturbed_doc}}\n",
    "Rationale:{{perturbed_response | replace('\\n', '')}}<end>\n",
    "\n",
    "Please explain:\n",
    "Question:{{question}} \n",
    "Choices:{{choices.text}}\n",
    "Rationale:\n",
    "\"\"\"\n",
    "\n",
    "inferdpt_client = inferdpt.InferDPTClient(ctx, kit, inference, epsilon=3.0)\n",
    "result = inferdpt_client.inference([test_example], doc_template, instruction_template, decode_template, \\\n",
    "                                 remote_inference_kwargs={\n",
    "                                    'stop': ['<\\s>'],\n",
    "                                    'temperature': 0.01,\n",
    "                                    'max_tokens': 256\n",
    "                                 },\n",
    "                                 local_inference_kwargs={\n",
    "                                    'stop': ['<|im_end|>', '<end>', '<end>\\n', '<end>\\n\\n', '.\\n\\n\\n\\n\\n', '<|end_of_text|>', '>\\n\\n\\n'],\n",
    "                                    'temperature': 0.01,\n",
    "                                    'max_tokens': 256\n",
    "                                 })\n",
    "print('result is {}'.format(result[0]['inferdpt_result']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fbcb01-6907-432f-8393-ae1746559c3a",
   "metadata": {},
   "source": [
    "#### Server Side Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "960a476c-50a5-40fb-847d-02101cea27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fate_llm.algo.inferdpt.utils import InferDPTKit\n",
    "from fate_llm.algo.inferdpt.inferdpt import InferDPTClient, InferDPTServer\n",
    "from jinja2 import Template\n",
    "from fate.arch import Context\n",
    "import sys\n",
    "from fate_llm.algo.inferdpt.inference.api import APICompletionInference\n",
    "\n",
    "arbiter = (\"arbiter\", 10000)\n",
    "guest = (\"guest\", 10000)\n",
    "host = (\"host\", 9999)\n",
    "name = \"fed1\"\n",
    "\n",
    "def create_ctx(local):\n",
    "    from fate.arch import Context\n",
    "    from fate.arch.computing.backends.standalone import CSession\n",
    "    from fate.arch.federation.backends.standalone import StandaloneFederation\n",
    "    import logging\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(console_handler)\n",
    "    computing = CSession(data_dir=\"./session_dir\")\n",
    "    return Context(computing=computing, federation=StandaloneFederation(computing, name, local, [guest, host, arbiter]))\n",
    "\n",
    "ctx = create_ctx(arbiter)\n",
    "# Api to a LLM\n",
    "inference_server = APICompletionInference(api_url=\"http://127.0.0.1:8888/v1\", model_name='./Mistral-7B-Instruct-v0.2', api_key='EMPTY')\n",
    "inferdpt_server = InferDPTServer(ctx, inference_server)\n",
    "inferdpt_server.inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f908a7-9187-461a-93db-9945456d502d",
   "metadata": {},
   "source": [
    "Start two terminal and launch client&server scripts simultaneously. On the client side we can get the answer:\n",
    "\n",
    "```\n",
    "The given question asks which factor will most likely cause a person to develop a fever. The factors mentioned are a leg muscle relaxing after exercise, a bacterial population in the bloodstream, several viral particles on the skin, and carbohydrates being digested in the stomach. The question is asking which factor is most likely to cause a person to develop a fever. The factors are all related to the body's internal environment, but the most likely factor is a bacterial population in the bloodstream. This is because bacteria can cause a fever, and the body's immune system responds to the infection by producing antibodies that can fight off the bacteria. Therefore, the answer is 'a bacterial population in the bloodstream'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36a485-2fa8-4629-a2cf-2d53fdbbcc5f",
   "metadata": {},
   "source": [
    "### The Encoder-Decoder Model Strategy\n",
    "\n",
    "Similar to the InferDPT, we can initialize SLMEncoderDecoderClient and SLMEncoderDecoderServer to enable secure inference.\n",
    "The client will encode the raw prompt using local slm model and then decoded it with the same model\n",
    "\n",
    "#### Client Side Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd174244-8640-4cb2-8609-ac6468f5a6f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fate_llm.algo.inferdpt.inference.api import APICompletionInference\n",
    "from fate_llm.algo.pdss.encoder_decoder.slm_encoder_decoder import SLMEncoderDecoderClient\n",
    "\n",
    "arbiter = (\"arbiter\", 10000)\n",
    "guest = (\"guest\", 10000)\n",
    "host = (\"host\", 9999)\n",
    "name = \"fed1\"\n",
    "\n",
    "def create_ctx(local):\n",
    "    from fate.arch import Context\n",
    "    from fate.arch.computing.backends.standalone import CSession\n",
    "    from fate.arch.federation.backends.standalone import StandaloneFederation\n",
    "    import logging\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(console_handler)\n",
    "    computing = CSession(data_dir=\"./session_dir\")\n",
    "    return Context(computing=computing, federation=StandaloneFederation(computing, name, local, [guest, host, arbiter]))\n",
    "\n",
    "\n",
    "test_example = {'id': 'Mercury_7220990',\n",
    "'question': 'Which factor will most likely cause a person to develop a fever?',\n",
    "'choices': {'text': ['a leg muscle relaxing after exercise',\n",
    "'a bacterial population in the bloodstream',\n",
    "'several viral particles on the skin',\n",
    "'carbohydrates being digested in the stomach'],\n",
    "'label': ['A', 'B', 'C', 'D']},\n",
    "'answerKey': 'B'\n",
    "}\n",
    "\n",
    "\n",
    "encode_prompt = \"\"\"Disrupt the main words in the original text so that it becomes difficult to recognize, but at the same time, try to maintain the original meaning as much as possible. Use <end> to end your reply.\n",
    "Origin Doc:Question:{{question}}\n",
    "Choices:{{choices.text}}\n",
    "Perturb Doc: \n",
    "\"\"\"\n",
    "\n",
    "decode_prompt = \"\"\"This is a perturbed question and its corresponding answer(rationale). And following is the original question. Try to recover the correct rationale from docs provided.\n",
    "\n",
    "Perturbed doc and rationale:\n",
    "{{perturbed_doc}}\n",
    "Rationale:{{perturbed_response}}\n",
    "\n",
    "Original Doc:\n",
    "Question:{{question}}\n",
    "Choices:{{choices.text}}\n",
    "\n",
    "Recover Rationale:\n",
    "\"\"\"\n",
    "\n",
    "instruction_template = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant<|im_end|>\n",
    "<|im_start|>user\n",
    "Select Answer from Choices and explain it in \"Rationale\" with few words. Please refer to the example to write the rationale.\n",
    "Use <end> to finish your rationle.\n",
    "\n",
    "Example(s):\n",
    "Question:Which factor will most likely cause a person to develop a fever?\n",
    "Choices:['a leg muscle relaxing after exercise', 'a bacterial population in the bloodstream', 'several viral particles on the skin', 'carbohydrates being digested in the stomach']\n",
    "Rationale:A bacterial infection in the bloodstream triggers the immune system to respond, therefore often causing a fever as the body tries to fight off the bacteria. Therefore, the answer is 'a bacterial population in the bloodstream'\n",
    "\n",
    "Please explain:\n",
    "{{perturbed_doc}}\n",
    "Rationale:\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "ctx = create_ctx(guest)\n",
    "model_name = 'Deploy your encoder decoder model'\n",
    "# api_url to your locally deployed encoder decoder\n",
    "api = APICompletionInference(api_url='http://127.0.0.1:8887/v1', api_key='EMPTY', model_name=model_name)\n",
    "client = SLMEncoderDecoderClient(ctx, api)\n",
    "result = client.inference([test_example], encode_prompt, instruction_template, decode_prompt, \\\n",
    "                                 remote_inference_kwargs={\n",
    "                                    'stop': ['<\\s>'],\n",
    "                                    'temperature': 0.01,\n",
    "                                    'max_tokens': 256\n",
    "                                 },\n",
    "                                 local_inference_kwargs={\n",
    "                                    'stop': ['<|im_end|>', '<end>', '<end>\\n', '<end>\\n\\n', '.\\n\\n\\n\\n\\n', '<|end_of_text|>', '>\\n\\n\\n'],\n",
    "                                    'temperature': 0.01,\n",
    "                                    'max_tokens': 256\n",
    "                                 })\n",
    "print('result is {}'.format(result[0]['inferdpt_result']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a865536-7814-40a2-a814-d00e46f2787f",
   "metadata": {},
   "source": [
    "#### Server Side Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cced44b0-0dcb-4427-8efe-a04135b246ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fate_llm.algo.inferdpt.inference.api import APICompletionInference\n",
    "from fate_llm.algo.pdss.encoder_decoder.slm_encoder_decoder import SLMEncoderDecoderServer\n",
    "\n",
    "arbiter = (\"arbiter\", 10000)\n",
    "guest = (\"guest\", 10000)\n",
    "host = (\"host\", 9999)\n",
    "name = \"fed1\"\n",
    "\n",
    "def create_ctx(local):\n",
    "    from fate.arch import Context\n",
    "    from fate.arch.computing.backends.standalone import CSession\n",
    "    from fate.arch.federation.backends.standalone import StandaloneFederation\n",
    "    import logging\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(console_handler)\n",
    "    computing = CSession(data_dir=\"./session_dir\")\n",
    "    return Context(computing=computing, federation=StandaloneFederation(computing, name, local, [guest, host, arbiter]))\n",
    "\n",
    "ctx = create_ctx(arbiter)\n",
    "# api url&name are depolyed LLM\n",
    "model_name = '/data/cephfs/llm/models/Qwen1.5-14B-Chat/'\n",
    "api = APICompletionInference(api_url='http://127.0.0.1:8888/v1', api_key='EMPTY', model_name=model_name)\n",
    "server = SLMEncoderDecoderServer(ctx, api)\n",
    "server.inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ed7a6-2eb2-4f46-b59c-eaafcc9a5b7a",
   "metadata": {},
   "source": [
    "Start two terminal and launch client&server scripts simultaneously. On the client side we can get the answer:\n",
    "\n",
    "```\n",
    "A fever is typically caused by a bacterial population in the bloodstream, as it is a response to an infection. So the answer is 'a bacterial population in the bloodstream'.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbbefd-e931-4e95-9d28-9675ff7865a3",
   "metadata": {},
   "source": [
    "## Prefix Dataset & PDSS Trainer\n",
    "\n",
    "Now that we can carry out privacy-preserving inference and acquire rationales, the next step is to train a new task-specific model, enhanced by the rationales generated by the LLMs.\n",
    "\n",
    "In this section, we will introduce the PrefixDataset and PDSSTrainer, which facilitate training tasks with the added benefit of supplementary rationales. The PrefixDataset allows you to assign various text prefixes, guiding the model to produce different text targets. With PDSSTrainer, the model is trained to generate both text labels and text rationales at each update step, ultimately leading to superior performance compared to training on the raw dataset alone.\n",
    "\n",
    "### Prepare dataset\n",
    "In this tutorial, we will use the arc-easy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25377d0-1a7e-4e8c-aa9f-3bcb03ae0c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"arc_easy\")\n",
    "dataset.save_to_disk('path_to_save/arce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9166110f-bf67-4bf1-9da8-04c16bd79423",
   "metadata": {},
   "source": [
    "Let’s proceed with testing the PrefixDataset. We can utilize Jinja2 templates to structure the text and append prefixes or suffixes to our training data.\n",
    "\n",
    "Please note that at this stage, the dataset does not contain rationales. In the 'rationale_output_template', the key used for the inference results is ‘infer_result’. We can perform secure inference using the PDSSTrainer and then integrate the rationale results, keyed as ‘infer_result’, into the PrefixDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdbd93d6-45f3-404f-813e-9ca1fd6def04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from fate_llm.dataset.pdss_dataset import PrefixDataset\n",
    "\n",
    "pds = PrefixDataset(\n",
    "        tokenizer_path='/data/cephfs/llm/models/Qwen1.5-0.5B/',\n",
    "        predict_input_template=\"\"\"Predict:\n",
    "Question:{{question}}\n",
    "Choices:{{choices.text}}\n",
    "Answer:\n",
    "    \"\"\",\n",
    "        predict_output_template=\"\"\"{{choices.text[choices.label.index(answerKey)]}}<end>\"\"\",\n",
    "        rationale_input_template=\"\"\"Explain:\n",
    "Question:{{question}}\n",
    "Choices:{{choices.text}}\n",
    "Rationale:\n",
    "    \"\"\",\n",
    "        rationale_output_template=\"\"\"{{infer_result}}<end>\"\"\",\n",
    "        max_input_length=128,\n",
    "        max_target_length=128,\n",
    "        split_key='train'\n",
    "    )\n",
    "\n",
    "\n",
    "pds.load('path_to_save/arce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "100eeb69-8bd2-4e66-b1cc-667f95e47f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Mercury_7220990',\n",
       " 'question': 'Which factor will most likely cause a person to develop a fever?',\n",
       " 'choices': {'text': ['a leg muscle relaxing after exercise',\n",
       "   'a bacterial population in the bloodstream',\n",
       "   'several viral particles on the skin',\n",
       "   'carbohydrates being digested in the stomach'],\n",
       "  'label': ['A', 'B', 'C', 'D']},\n",
       " 'answerKey': 'B'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pds.dataset[0] # the structure is the same as hf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f0356ef-f94b-41db-ab66-b1d0eb862eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predict': {'input': \"Predict:\\nQuestion:Which factor will most likely cause a person to develop a fever?\\nChoices:['a leg muscle relaxing after exercise', 'a bacterial population in the bloodstream', 'several viral particles on the skin', 'carbohydrates being digested in the stomach']\\nAnswer:\\n    \",\n",
       "  'output': 'a bacterial population in the bloodstream<end>'},\n",
       " 'rationale': {'input': \"Explain:\\nQuestion:Which factor will most likely cause a person to develop a fever?\\nChoices:['a leg muscle relaxing after exercise', 'a bacterial population in the bloodstream', 'several viral particles on the skin', 'carbohydrates being digested in the stomach']\\nRationale:\\n    \",\n",
       "  'output': '<end>\\n    '}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pds.get_str_item(0)  # we can see that the output of rationale term is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a227af7-f24a-46bd-9af7-78584a381b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pds[0]) # show tokenized, for the sake of breif we dont show it in this tutorial doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0382a33-7a45-43a3-8ed3-58ed1d1b07d8",
   "metadata": {},
   "source": [
    "### The PDSSTrainer\n",
    "\n",
    "Here we introduce the PDSSTrainer which is develop based on Huggingface trainer and supports collaboratively training a task with raw labels and additional rationales. Here show how the compute loss function is realized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b7d99-9ef8-43f9-8e28-db96d96af62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "    label_outputs = model(**inputs['predict'])\n",
    "    cot_outputs = model(**inputs['rationale'])\n",
    "    loss = self.alpha * cot_outputs.loss + (1. - self.alpha) * label_outputs.loss\n",
    "    return (loss, {'rationale_loss': cot_outputs, 'predict_loss': label_outputs}) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1cee5d-68e1-4caf-96b9-132b27b46dca",
   "metadata": {},
   "source": [
    "You have the option to choose from three distinct modes: ‘infer_only’, ‘train_only’, and ‘infer_and_train’, to meet your specific requirements.\n",
    "- infer_only: Only generate the rationales and they will be saved to the output_dir\n",
    "- train_only: Local training only\n",
    "- infer_and_train: Generate rationales, and then load them into PrefixDataset and start training\n",
    "  \n",
    "In this instance, we will opt for the ‘infer_and_train’ mode to initially generate rationales with the assistance of the remote LLM. To activate the inference process, it is necessary to initialize the infer client and server for both the client-side and server-side trainers, as demonstrated in the preceding sections.\n",
    "\n",
    "Below is an PDSS example. We ran this example on a machine equipped with 4 V100-32G GPUs. We launch the client script using deepspeed. LLM is depolyed on another machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559341a-d133-4a24-8f1a-35cd6d2a26d3",
   "metadata": {},
   "source": [
    "## PDSS Example\n",
    "\n",
    "### Client Script(deepspeed_run.py)\n",
    "\n",
    "This script show how to setup a pdss task on the client side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4710fda-904a-4e90-bc65-beec7594703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List\n",
    "from fate_llm.algo.inferdpt.utils import InferDPTKit\n",
    "from fate_llm.dataset.pdss_dataset import PrefixDataset\n",
    "from fate_llm.algo.pdss.pdss_trainer import PDSSTrainerClient\n",
    "from fate_llm.data.data_collator.pdss_collator import PrefixDataCollator\n",
    "from fate_llm.algo.inferdpt import inferdpt\n",
    "\n",
    "\n",
    "arbiter = (\"arbiter\", 10000)\n",
    "guest = (\"guest\", 10000)\n",
    "host = (\"host\", 9999)\n",
    "name = \"fed1\"\n",
    "\n",
    "\n",
    "def create_ctx(local):\n",
    "    from fate.arch import Context\n",
    "    from fate.arch.computing.backends.standalone import CSession\n",
    "    from fate.arch.federation.backends.standalone import StandaloneFederation\n",
    "    import logging\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(console_handler)\n",
    "    computing = CSession(data_dir=\"./session_dir\")\n",
    "    return Context(computing=computing, federation=StandaloneFederation(computing, name, local, [guest, host, arbiter]))\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "doc_template = \"\"\"{{question}} \n",
    "Choices:{{choices.text}}\n",
    "\"\"\"\n",
    "\n",
    "instruction_template=\"\"\"\n",
    "<s>[INST]\n",
    "Select Answer from Choices and explain it in \"Rationale\" with few words. Please refer to the example to write the rationale.\n",
    "Use <end> to finish your rationle.\"\n",
    "\n",
    "Example(s):\n",
    "Question:George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?\n",
    "Choices:['dry palms', 'wet palms', 'palms covered with oil', 'palms covered with lotion']\n",
    "Rationale:Friction between two surfaces generates heat due to the conversion of kinetic energy into thermal energy. Dry palms produce the most heat when rubbed together as they create higher friction compared to wet or lubricated palms, which reduce friction.  Therefore, the answer is 'dry palms'.<end>\n",
    "\n",
    "Please explain:\n",
    "Question:{{perturbed_doc}}\n",
    "Rationale:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "decode_template = \"\"\"Select Answer from Choices and explain it in \"Rationale\" with few words. Please refer to the example to write the rationale.\n",
    "Use <end> to finish your rationle.\"\n",
    "\n",
    "Example(s):\n",
    "Question:George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?\n",
    "Choices:['dry palms', 'wet palms', 'palms covered with oil', 'palms covered with lotion']\n",
    "Rationale:Friction between two surfaces generates heat due to the conversion of kinetic energy into thermal energy. Dry palms produce the most heat when rubbed together as they create higher friction compared to wet or lubricated palms, which reduce friction.  Therefore, the answer is 'dry palms'.<end>\n",
    "\n",
    "Question:{{perturbed_doc}}\n",
    "Rationale:{{perturbed_response | replace('\\n', '')}}<end>\n",
    "\n",
    "Please explain:\n",
    "Question:{{question}} \n",
    "Choices:{{choices.text}}\n",
    "Rationale:\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = HfArgumentParser(Seq2SeqTrainingArguments)\n",
    "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "        training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))[0]\n",
    "    else:\n",
    "        training_args = parser.parse_args_into_dataclasses()[0]\n",
    "\n",
    "    model_path = '/data/cephfs/llm/models/Qwen1.5-0.5B/'\n",
    "    pds = PrefixDataset(\n",
    "        tokenizer_path=model_path,\n",
    "        predict_input_template=\"\"\"Predict:\n",
    "Question:{{question}}\n",
    "Choices:{{choices.text}}\n",
    "Answer:\n",
    "    \"\"\",\n",
    "        predict_output_template=\"\"\"{{choices.text[choices.label.index(answerKey)]}}<end>\"\"\",\n",
    "        rationale_input_template=\"\"\"Explain:\n",
    "Question:{{question}}\n",
    "Choices:{{choices.text}}\n",
    "Rationale:\n",
    "    \"\"\",\n",
    "        rationale_output_template=\"\"\"{{infer_result}}<end>\n",
    "    \"\"\",\n",
    "        max_input_length=128,\n",
    "        max_target_length=128,\n",
    "        split_key='train'\n",
    "    )\n",
    "    pds.load('/data/cephfs/llm/datasets/arce/')\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path).half().cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    ctx = create_ctx(guest)\n",
    "    if training_args.local_rank == 0:\n",
    "        # only rank 0 need to load infer instance\n",
    "        save_kit_path = 'your path'\n",
    "        kit = InferDPTKit.load_from_path(save_kit_path)\n",
    "        # local deployed small model as decoding model\n",
    "        from fate_llm.algo.inferdpt.inference.api import APICompletionInference\n",
    "        inference = APICompletionInference(api_url=\"http://xxxx/v1\", model_name='./Qwen1.5-0.5B', api_key='EMPTY')\n",
    "        client = inferdpt.InferDPTClient(ctx, kit, inference, epsilon=3.0)\n",
    "    else:\n",
    "        client = None\n",
    "    \n",
    "    trainer = PDSSTrainerClient(\n",
    "        ctx=ctx,\n",
    "        model=model,\n",
    "        training_args=training_args,\n",
    "        tokenizer=tokenizer,    \n",
    "        train_set=pds,\n",
    "        data_collator=PrefixDataCollator(tokenizer),\n",
    "        mode='infer_and_train',\n",
    "        infer_client=client,\n",
    "        encode_template=doc_template,\n",
    "        decode_template=decode_template,\n",
    "        instruction_template=instruction_template,\n",
    "        remote_inference_kwargs={\n",
    "            'stop': ['<\\s>'],\n",
    "            'temperature': 0.01,\n",
    "            'max_tokens': 256\n",
    "         },\n",
    "         local_inference_kwargs={\n",
    "            'stop': ['<|im_end|>', '<end>', '<end>\\n', '<end>\\n\\n', '.\\n\\n\\n\\n\\n', '<|end_of_text|>', '>\\n\\n\\n'],\n",
    "            'temperature': 0.01,\n",
    "            'max_tokens': 256\n",
    "         }\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    if training_args.local_rank == 0:\n",
    "        model.save_pretrained(training_args.output_dir)\n",
    "        tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962dd399-1dec-4164-bd86-15aa8550c50b",
   "metadata": {},
   "source": [
    "### Server Script(server.py)\n",
    "\n",
    "This script show how to setup a pdss task on the server side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b42972-5308-4ccf-a768-f7dfa087313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fate_llm.algo.inferdpt.inferdpt import InferDPTServer\n",
    "from fate_llm.algo.pdss.pdss_trainer import PDSSTraineServer\n",
    "from jinja2 import Template\n",
    "from fate.arch import Context\n",
    "import sys\n",
    "\n",
    "\n",
    "arbiter = (\"arbiter\", 10000)\n",
    "guest = (\"guest\", 10000)\n",
    "host = (\"host\", 9999)\n",
    "name = \"fed1\"\n",
    "\n",
    "\n",
    "def create_ctx(local):\n",
    "    from fate.arch import Context\n",
    "    from fate.arch.computing.backends.standalone import CSession\n",
    "    from fate.arch.federation.backends.standalone import StandaloneFederation\n",
    "    import logging\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(console_handler)\n",
    "    computing = CSession(data_dir=\"./session_dir\")\n",
    "    return Context(computing=computing, federation=StandaloneFederation(computing, name, local, [guest, host, arbiter]))\n",
    "\n",
    "\n",
    "from fate_llm.algo.inferdpt.inference.api import APICompletionInference\n",
    "api = APICompletionInference(api_url='http://xxxx:8080/v1', api_key='EMPTY', model_name='/data/cephfs/llm/models/Qwen1.5-14B-Chat')\n",
    "\n",
    "ctx = create_ctx(arbiter)\n",
    "server_api = InferDPTServer(ctx, api)\n",
    "server = PDSSTraineServer(ctx, server_api)\n",
    "server.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125dd68e-c7d4-41aa-9972-4881b1330fb6",
   "metadata": {},
   "source": [
    "### Start script\n",
    "\n",
    "You can launch client side training with following script:\n",
    "\n",
    "```\n",
    "deepspeed --num_nodes 1 --num_gpus 4 deepspeed_run.py \\\n",
    "    --output_dir \"./\" \\\n",
    "    --per_device_train_batch_size \"1\" \\\n",
    "    --gradient_accumulation_steps \"8\" \\\n",
    "    --max_steps \"750\" \\\n",
    "    --fp16 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_only_model \\\n",
    "    --deepspeed \"./ds_config.json\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b506c1c-51f4-448d-9b0b-adf1a71cc7cf",
   "metadata": {},
   "source": [
    "and the ds_config.json is\n",
    "```\n",
    "{   \n",
    "    \"train_micro_batch_size_per_gpu\": 1,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "             \"lr\": 5e-5\n",
    "        }\n",
    "    },\n",
    "    \"fp16\": {\n",
    "        \"enabled\": true\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 0\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613fbfb6-ac9e-485b-8587-ffef1e2361c1",
   "metadata": {},
   "source": [
    "And server side:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b50adf0-8f9c-40e5-9a7d-40a70e30a420",
   "metadata": {},
   "source": [
    "```python server.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5de71-25fd-4042-a6b7-0ec2c505eaee",
   "metadata": {},
   "source": [
    "## PDSS Pipeline Example\n",
    "\n",
    "You have the capability to submit a PDSS task within the FATE pipeline. By appropriately configuring the necessary settings, you can execute PDSS in a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1e19b-da8e-4977-adb1-42fb84dee407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fate_llm.runner.pdss_runner import PDSSRunner\n",
    "from fate.components.components.nn.nn_runner import loader_load_from_conf\n",
    "from fate.components.components.nn.loader import Loader\n",
    "from fate_llm.dataset.pdss_dataset import PrefixDataset\n",
    "from fate_client.pipeline.components.fate.nn.loader import ModelLoader, DatasetLoader, CustFuncLoader, Loader\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    "    Trainer\n",
    ")\n",
    "import argparse\n",
    "from fate_client.pipeline.utils import test_utils\n",
    "from fate_client.pipeline.components.fate.evaluation import Evaluation\n",
    "from fate_client.pipeline.components.fate.reader import Reader\n",
    "from fate_client.pipeline import FateFlowPipeline\n",
    "from fate_client.pipeline.components.fate.nn.torch import nn, optim\n",
    "from fate_client.pipeline.components.fate.nn.torch.base import Sequential\n",
    "from fate_client.pipeline.components.fate.homo_nn import HomoNN, get_config_of_default_runner\n",
    "from fate_client.pipeline.components.fate.nn.algo_params import TrainingArguments, FedAVGArguments\n",
    "\n",
    "def main(config=\"../../config.yaml\", namespace=\"\"):\n",
    "    # obtain config\n",
    "    if isinstance(config, str):\n",
    "        config = test_utils.load_job_config(config)\n",
    "    parties = config.parties\n",
    "    guest = '9999'\n",
    "    host = parties.host[0]\n",
    "    arbiter = '10000'\n",
    "\n",
    "    pipeline = FateFlowPipeline().set_parties(guest=guest, arbiter=arbiter)\n",
    "\n",
    "    reader_0 = Reader(\"reader_0\", runtime_parties=dict(guest=guest))\n",
    "    reader_0.guest.task_parameters(\n",
    "        namespace=\"experiment\",\n",
    "        name=\"arc_e_example\"\n",
    "    )\n",
    "\n",
    "    model_conf = Loader(module_name='fate_llm.model_zoo.hf_model', item_name='HFAutoModelForCausalLM', \n",
    "                        pretrained_model_name_or_path='/data/cephfs/llm/models/Qwen1.5-0.5B/').to_dict()\n",
    "    data_collator_conf = Loader(module_name='fate_llm.data.data_collator.pdss_collator', item_name='get_prefix_data_collator', tokenizer_name_or_path='/data/cephfs/llm/models/Qwen1.5-0.5B/').to_dict()\n",
    "\n",
    "    infer_init_conf_client = {\n",
    "        'module_name': 'fate_llm.algo.inferdpt.init.default_init',\n",
    "        'item_name': 'InferDPTAPIClientInit'\n",
    "    }\n",
    "\n",
    "    infer_init_conf_server = {\n",
    "        'module_name': 'fate_llm.algo.inferdpt.init.default_init',\n",
    "        'item_name': 'InferDPTAPIServerInit'\n",
    "    }\n",
    "\n",
    "    dataset_conf = {\n",
    "        'module_name': 'fate_llm.dataset.pdss_dataset',\n",
    "        'item_name': 'PrefixDataset',\n",
    "        'kwargs':dict(\n",
    "            tokenizer_path='/data/cephfs/llm/models/Qwen1.5-0.5B/',\n",
    "            predict_input_template=\"\"\"Predict:\n",
    "    Question:{{question}}\n",
    "    Choices:{{choices.text}}\n",
    "    \"\"\",\n",
    "            predict_output_template=\"\"\"{{choices.text[choices.label.index(answerKey)]}}<end>\"\"\",\n",
    "            rationale_input_template=\"\"\"Explain:\n",
    "    Question:{{question}}\n",
    "    Choices:{{choices.text}}\n",
    "    \"\"\",\n",
    "            rationale_output_template=\"\"\"{{infer_result}}<end>\n",
    "        \"\"\",\n",
    "            max_input_length=128,\n",
    "            max_target_length=128,\n",
    "            split_key='train'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    encoder_prompt = \"\"\"{{question}}\n",
    "Choices:{{choices.text}}\n",
    "\"\"\"\n",
    "\n",
    "    decoder_prompt = \"\"\"Select Answer from Choices and explain it in \"Rationale\" with few words. Please refer to the example to write the rationale.Use <end> to finish your rationle.\n",
    "\n",
    "Example(s):\n",
    "Question:George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?\n",
    "Choices:['dry palms', 'wet palms', 'palms covered with oil', 'palms covered with lotion']\n",
    "Rationale:Friction between two surfaces generates heat due to the conversion of kinetic energy into thermal energy. Dry palms produce the most heat when rubbed together as they create higher friction compared to wet or lubricated palms, which reduce friction.  Therefore, the answer is 'dry palms'.<end>\n",
    "\n",
    "Question:{{perturbed_doc}}\n",
    "Rationale:{{perturbed_response | replace('\\n', '')}}<end>\n",
    "\n",
    "Please explain:\n",
    "Question:{{question}} \n",
    "Choices:{{choices.text}}\n",
    "    \"\"\"\n",
    "\n",
    "    instruction_prompt = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant<|im_end|>\n",
    "<|im_start|>user\n",
    "Select Answer from Choices and explain it in \"Rationale\" with few words. Please refer to the example to write the rationale.\n",
    "Use <end> to finish your rationle.\n",
    "\n",
    "Example(s):\n",
    "Question:Which factor will most likely cause a person to develop a fever?\n",
    "Choices:['a leg muscle relaxing after exercise', 'a bacterial population in the bloodstream', 'several viral particles on the skin', 'carbohydrates being digested in the stomach']\n",
    "Rationale:A bacterial infection in the bloodstream triggers the immune system to respond, therefore often causing a fever as the body tries to fight off the bacteria. Therefore, the answer is 'a bacterial population in the bloodstream'\n",
    "\n",
    "Please explain:\n",
    "Question:{{perturbed_doc}}\n",
    "Rationale:\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "    \"\"\"\n",
    "\n",
    "    remote_inference_kwargs={\n",
    "        'stop': ['<|im_end|>', '<end>', '<end>\\n', '<end>\\n\\n', '.\\n\\n\\n\\n\\n', '<|end_of_text|>', '>\\n\\n\\n'],\n",
    "        'temperature': 0.01,\n",
    "        'max_tokens': 256\n",
    "    }\n",
    "\n",
    "    local_inference_kwargs={\n",
    "        'stop': ['<|im_end|>', '<end>', '<end>\\n', '<end>\\n\\n', '.\\n\\n\\n\\n\\n', '<|end_of_text|>', '>\\n\\n\\n'],\n",
    "        'temperature': 0.01,\n",
    "        'max_tokens': 256\n",
    "    }\n",
    "\n",
    "    ds_config = {   \n",
    "        \"train_micro_batch_size_per_gpu\": 1,\n",
    "        \"gradient_accumulation_steps\": 8,\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\n",
    "                \"lr\": 5e-5\n",
    "            }\n",
    "        },\n",
    "        \"fp16\": {\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    training_args_dict = dict(\n",
    "        per_device_train_batch_size=1, \n",
    "        gradient_accumulation_steps=8,\n",
    "        logging_steps=10,\n",
    "        max_steps=30,\n",
    "        fp16=True,\n",
    "        log_level='debug'\n",
    "    )\n",
    "\n",
    "    mode = 'infer_and_train'\n",
    "\n",
    "    client_conf = dict(\n",
    "        model_conf=model_conf,\n",
    "        dataset_conf=dataset_conf,\n",
    "        training_args_conf=training_args_dict,\n",
    "        data_collator_conf=data_collator_conf,\n",
    "        mode=mode,\n",
    "        infer_inst_init_conf=infer_init_conf_client,\n",
    "        encode_template=encoder_prompt,\n",
    "        instruction_template=instruction_prompt,\n",
    "        decode_template=decoder_prompt,\n",
    "        remote_inference_kwargs=remote_inference_kwargs,\n",
    "        local_inference_kwargs=local_inference_kwargs,\n",
    "        perturb_doc_key='perturbed_doc',\n",
    "        perturbed_response_key='perturbed_response',\n",
    "        result_key='infer_result'\n",
    "    )\n",
    "\n",
    "    server_conf = dict(\n",
    "        infer_inst_init_conf=infer_init_conf_server,\n",
    "        mode=mode\n",
    "    )\n",
    "\n",
    "    homo_nn_0 = HomoNN(\n",
    "        'nn_0',\n",
    "        train_data=reader_0.outputs[\"output_data\"],\n",
    "        runner_module=\"pdss_runner\",\n",
    "        runner_class=\"PDSSRunner\"\n",
    "    )\n",
    "\n",
    "    homo_nn_0.guest.task_parameters(runner_conf=client_conf)\n",
    "    homo_nn_0.arbiter.task_parameters(runner_conf=server_conf)\n",
    "\n",
    "    homo_nn_0.guest.conf.set(\"launcher_name\", \"deepspeed\")\n",
    "\n",
    "    pipeline.add_tasks([reader_0, homo_nn_0])\n",
    "    pipeline.conf.set(\"task\", dict(engine_run={\"cores\": 4}))\n",
    "    pipeline.compile()\n",
    "    pipeline.fit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\"PIPELINE DEMO\")\n",
    "    parser.add_argument(\"--config\", type=str, default=\"../config.yaml\",\n",
    "                        help=\"config file\")\n",
    "    parser.add_argument(\"--namespace\", type=str, default=\"\",\n",
    "                        help=\"namespace for data stored in FATE\")\n",
    "    args = parser.parse_args()\n",
    "    main(config=args.config, namespace=args.namespace)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
